Previously, I talked about the three insights I gained from Josh Wills, Clouderas Director of Data Science, in preparation of the Rethink Analytics, with an Enterprise Data Hub webinar. In addition to my personal revelation during the preparation of the webinar, we also received a number of great questions from the webinar audience.Is an EDH available now?An EDH, short for enterprise data hub, is one place to store all data, for as long as desired or required, in its original fidelity; integrated with existing infrastructure and tools; with the flexibility to run a variety of enterprise workloads  including batch processing, interactive SQL, enterprise search and advanced analytics  together with the robust security, governance, data protection, and management that enterprises require.Clouderas support for an EDH is through our enterprise-grade product, Cloudera Enterprise, which is currently available for purchase.What internal tools are used for analytics in EDH?Every data scientist and every data science team has their own preferred stack of tools to use for advanced analytics on an EDH. There are many tools, ranging from BI to machine learning algorithms. However, the key is to use the right tool for the job. Josh uses a lot of R, SAS, Python, etc., but he makes sure that he is choosing the tool that fits with customer requirements, and the tool that is appropriate for the data he is working with at the time.What makes an EDH and Hadoop so exciting is that every single one of these tools, whether its an existing machine learning toolkit or one of a number of new entrants, is all re-orienting themselves around Hadoop. They have realized that Hadoop is where a lot of the data is and where more of the data is going in the future. And thanks to the flexibility of an EDH, these tools can easily be designed and integrated into this data platform.What skillsets are necessary to enable the success of using an EDH for advanced analytics?Similar to the question about which tool to use when doing advanced analytics, it is not a question of any one skillset, but more about having the ability to think and reason in a more advanced, complex, interesting way to structure data for analytics.Josh recommends Clouderas training class, Introduction to Data Science. This course provides an overview of recommendation systems and tools and covers a lot of machine learning algorithms, but the primary purpose of the class is to introduce students to the way a seasoned data scientist thinks about data modeling in Hadoop. How the data scientist is going to structure the data, in such a way that they can optimize the overall analytical pipeline and analytical workflow. It is really about the ability to learn and model the data for analysis.How do you manage data quality in an EDH architecture? What are the interactions in the flow that address data quality?Data quality, from a data scientists perspective, is to have the sampled data accurately reflect the real-world data environment that the data scientist is trying to build the model for. The good news in doing advanced analytics in a Hadoop environment is that the files on Hadoop are essentially immutable in nature; that is, these files are append-only, and therefore you cannot open up a file, make modifications, and then save the file, as you would on a laptop. With Cloudera Enterprise, we couple the immutable data with the ability to track data lineage, so when a data quality problem is found, you can go identify the problem and remove it from the analytical pipeline to prevent this data quality issue to propagate even further.What is Oryx and how does it differ from Apache Mahout?Oryx started when Cloudera acquired a company called Myrrix, which was founded by Sean Owen (now Director of Data Sciene for Cloudera in EMEA), a long time contributor to the Apache Mahout project. At the time of forming Myrrix, Sean wanted to focus on a small set of very well implemented algorithms designed specifically for recommendation engines. After the acquisition of Myrrix, Cloudera decided to open source the code and formed the Oryx project. As we covered briefly during our Rethink Analytics webinar, there is a significant gap between the building of the models and the deployment of the model. Oryx, differs from Mahout, focuses on not only the building of these models, but also the serving of these models. With Oryx, data scientists can pick up these models and serve them immediately into production.What impact will Spark have on an EDH?We are seeing that Spark is becoming the open-source in-memory analytics component in an EDH. Just as MapReduce has become the general-purpose distributed batch-processing model, we will see Spark become the general-purpose distributed in-memory analytics model.Right now, the Cloudera data science team is working on porting all the backend algorithms in Oryx over to Spark, to use for machine learning applications. Going forward, we foresee that analytical computing that is done in MapReduce right now will shift over to Spark in the next couple of years.Check out the Rethink Analytics, with an Enterprise Data Hub webinar replay Check out Cloudera Enterprise and download a trial of the Cloudera Enterprise Data Hub Edition During my preparation for the Rethink Analytics, with an Enterprise Data Hub webinar, I got together with Josh Wills, Clouderas Director of Data Science, to learn about why advanced analytics is so critical in todays world and how a unified data management platform, such as an enterprise data hub (EDH), is so critical.As with all of the conversations Ive had with Josh, I always come away with a couple of great insights. This time, I left the meeting with three:Change of ContextBusiness transactions have been going on since cavemen were bartering meat for seashells. So whats the big deal with big data? Why is there a sudden urgency in understanding these transactions, not only as individual ones, but also as a unified view of all transactions? It is because the context in which these transactions take place is very different and the rate at which the context changes is much more rapid. The introduction of the Internet, mobile devices, and the Internet-of-Things, has drastically changed the way we learn about products, the way we receive promotions, compare prices, the way we purchase (in-store vs. online), and even the way we pay.In the world of cavemen, or even the early part of our own generation, the purchase cycle was very lengthy and generally involved only brick-and-mortar stores. The education, promotion, stocking/re-stocking, and the selling of the goods and services could take anywhere from days to months. In contrast, purchasing behavior today is very rapid. Just a few days ago I learned about FitBits silent alarm feature from a blog article. That same evening I researched a little more on my phone while commuting via train, found out that the Target store a block away carries it at a good price, and was awakened the next morning by the pleasant buzzing of my own FitBit. Businesses need to anticipate the ever-changing context and be able to influence customers decision-making processes as quickly as possible.Tightening the OODA Loop with DataWar and business are often compared and contrasted. In our discussion, Josh introduced me to the concept of the OODA (Observe, Orient, Decide and Act) loop, a military decision-making theory created by former US Air Force Colonel John Boyd. The four stages of the OODA loops are:Observe  Collect data through all available sourcesOrient  Perform analysis of current situation based on data collectedDecide  Determine next course of action based on the current situationAct  Execute the decision made in previous phaseDuring the Korean War, Colonel Boyd observed that the American F-86 Sabre fighter planes were more successful in air-to-air combat against the Soviet MiG-15s. The reason for the success was that F-86 offered a much better field of vision, and therefore the pilots were able to gather information quicker, make sense of the situation sooner, and tighten the OODA loop.And that is the key for modern business to achieve success against competition as well. To quote Harry Hillaker, chief designer of the F-16, Time is the dominant parameter. The pilot who goes through the OODA cycle in the shortest time prevails because his opponent is caught responding to situations that have already changed. Relating it back to modern day businesses, it is all about the ability to shorten the observation and orientation stages, or data collection and analysis stages. The ability to turn the data collected into actionable insights ahead of competition is absolutely the key to success.Doing More, Faster with Cloudera EnterpriseBringing the context and the strategy together naturally raises the question of execution: How can Cloudera help to enable data scientists to make organizations more information-driven? Simply put, Cloudera Enterprise, our answer to an enterprise data hub solution, allows data practitioners to do more, faster with data.Use All Your Data  Cloudera Enterprise can accommodate more data and more types of data very economically and therefore reduce the need to limit or move large datasets, allowing data scientists to sample intelligently and at scale in order to increase model accuracy and precision. At the same time, it also provides centralized information security, metadata, management and governance capabilities.Shorten Analytics Lifecycle  As Josh mentioned in the webinar, data preparation is the bulk of the work when it comes to analytics. In order to shorten the analytics lifecycle, Cloudera Enterprise facilitates data discovery and tracks datas life-cycle in-place. Data practitioners can find the data they need and know where the data came from, through an interface or tool that is familiar to them, and ultimately shorten the data exploration and discovery phase and compress the cycle of time from data to insights from weeks to days, or even real-time.Do More with Data  Cloudera Enterprise allows the data practitioners to do more withdata while leveraging existing infrastructure and assets. Its ability to support multi-genre workloads ensures that organizations can continue to use existing tools (such as SAS, Revolution Analytics, Pentaho, Tableau, etc.) and leverage existing skillsets, while benefiting from a shared security, management and governance platform.How analytics is done is more or less the same, when it comes to the analytics lifecycle itself. However, with this unified data management platform, we can do much more than before and at a much faster rate. Ultimately, it shortens the observation and orientation phases of the OODA loop and allows the organization to be more agile, more information-driven, and more competitive.In the next post, we will go into some great questions that resulted in some also very insightful responses from Josh. Please stay tuned.Check out the Rethink Analytics, with an Enterprise Data Hub webinar replay Check out Cloudera Enterprise and download a trial of the Cloudera Enterprise Data Hub Edition Big Data has arguably been the most relevant story in IT during the past decade. The exponential growth of data and the information it holds areenabling new industries, creating new ways tomonetize good ideas, and helping older companiesget more agile, competitive, and market-driven.Cloudera strives to remove the barriers to data-driven business and create new ways for organizations to apply their data towards previously unimaginable outcomes. However, as companies begin to build out sophisticated enterprise data hubs with Hadoop at the core, they face a significant but not unexpected new challenge: a global shortage of talent. Data is plentiful, and the tools to leverage data as a differentiator are morepowerful,secure, and accessiblethan ever, but no organization can succeed without an experienced team of developers, administrators, and data analysts to execute its Big Data strategy.Thousands Trained, Millions to GoCloudera Universityhas been dedicated to educating data professionals on Hadoop since the very beginning and has trained and certified tens of thousands of people around the world. Still, the existing classroom model leaves two major gaps, in terms of the millions of skilled workers needed to grow a data economy built on the enterprise data hub (as projected byMcKinsey Global InstituteandSand Hill Group): massive global scale and early workforce preparedness.Our recentpartnership with Udacityhas been a successful step towards addressing the challenge of Hadoop education at scale. Since its launch two months ago,Introduction to Hadoop and MapReducehas helped more than 30,000 data enthusiasts get started on the Big Data career path. Now, anyone with an Internet connection, a few hours to spare, and a background in Python can learn to write basic MapReduce code.Although open online courses are a tremendous resource, they dont necessarily cater to the millions of young people who enter the workforce every year from colleges, universities, and trade schools. Because undergraduate education emphasizes broad technical capabilities, less so experience with the tools the marketplace requires, recent graduates tend to lack the most sought-after skills and are, thus, poor candidates for the most sought-after jobs. Compounding the problem, most professors, even from the top institutions, dont have the time or resources to learn and develop curricula on hot, new technologies like Hadoop.Thats why Cloudera continues to work with universities as part of theCloudera Academic Partnership(CAP). Since its inception in Spring 2013, 25 top American, European, and Asian institutions have joined CAP to facilitate the inclusion of Hadoop in their computer science, engineering, and advance analytics programs. CAP provides free access to Cloudera University course materials, classroom resources, and software and offers special discounts for teacher training, as well as access to a forum for instructors and curriculum developers.SJSU Grads Ride Hadoop to Job OppsDuring the past nine months, weve repeatedly heard from professors that Big Data and experience with the Hadoop stack are what set recent graduates apart in the eyes of hiring managers. Moreover, Cloudera Universitys approach to curriculum balances technical requirements, hands-on practice, and real-world scenarios such that participating students become good candidates for Cloudera Certification by the time they enter the workforce.The students enrolled in the Big Data Processing course at San Jose State University (SJSU) provide compelling evidence of CAPs impact. Peter Zadrozny, an adjunct professor in the SJSU Department of Computer Science and an accomplished software developer and architect in his own right, focuses his curriculum on discovery, management, and analysis of publicly available data sets using Hadoop and Hive. His students deploy Hadoop in the cloud using Cloudera Manager, then set out to find and explain the relationships between customer behaviors, marketing prompts, and environmental factors that would be valuable to companies today. Outstanding projects have included:Modeling campus safety based on geo-specific public crime records and macro-trends like drug enforcement policies, parking spot distribution, and construction projectsProjecting live event attendance vs. television viewership by analyzing the content, hashtags, geo-tags, and times of tweetsDetermining best options for updates to fast food menus by global region based on check-ins and geo-located tweetsZadroznys enthusiasm for CAP is pragmatic: I wanted to really teach students what Big Data is all about. Hiring managers want people that have gone through the steepest part of the learning curve. Ultimately, his students not only learn what Big Data is and understand Hadoop tools at a technical level, but also set up a cluster, load and query data, troubleshoot the instance, and present the results of their analyses as a final project. These last two elementsvalidation and visualizationdifferentiate the CAP experience for students because they are the skills that recruiters use to determine professional-grade Hadoop practitioners from hobbyists. With Clouderas help, SJSU is graduating computer scientists ready for jobs in Big Data and living up to its reputation of powering (and being empowered by) the best of the tech sector.See the full video success story and read the case study on SJSUs Big Data Processing course Learn how your college or alma mater can join the Cloudera Academic Partnership If you live in Europe and have ever bought a car, chances are that you used AutoScout24s online marketplace to research and maybe even purchase your vehicle. As one of the largest web properties in Europe, AutoScout24 serves more than ten million users across 18 European countries every single month. More than 40,000 car dealers leverage the service, which drives more than 500,000 monthly vehicle transactions.AutoScout24 has obviously found the right balance of information and advertisements  they serve plenty of valuable information to consumers eager to research the perfect car, and are smart about placing relevant ads in front of those users, driving revenue for the dealers that purchase digital ad space on the site.Theyve found this balance through an enterprise data hub (EDH) infrastructure that efficiently collects, processes, and enables data-driven action on massive volumes of weblog data generated throughout their website 247. AutoScout24 analyzes log files directly on Cloudera, and then publishes the results into relational systems which serve as back ends for their web platform.And recently, to optimize the performance and capabilities of their EDH, AutoScout24 turned to Cloudera Professional Services. The result? 10X faster performance, and the ability to analyze months of historical data in a single query (versus a few days before).This optimized EDH translates into better, faster, and more comprehensive business visibility which allows AutoScout24 to develop products that consumers care about, enhancing their experience and the efficacy of advertisements on the website.Learn more:Read the full case study: http://www.cloudera.com/content/dam/cloudera/Resources/PDF/casestudy/Cloudera_AutoScout24_CaseStudy_2014-01%20(4).pdfLearn more about AutoScout24: http://www.autoscout24.eu/Read about Clouderas Professional Services offerings: http://www.cloudera.com/content/cloudera/en/products-and-services/professional-services.htmlTheres no denying that big data is transforming the way which government operates. There are so many different applications and needs for big data analysis. The challenge of learning to manage the volume, variety and velocity of data is not going away anytime soon. In fact, this challenge is getting harder for agencies to tackle, as we are creating more data than ever before. The Cloudera Federal forum educated and trained government leaders on how to make smart big data investments and decisions. The event was moderated by Bob Gourley. Gourley is the publisher ofCTOvision.comandDelphiBrief.comand the new analysis focusedAnalyst One.Bobs background is as an all source intelligence analyst and an enterprise CTO. Find him on Twitter at@BobGourley, and be sure to follow his blog, hes always got great content and stories he is sharing. Gourley led us through an exciting day focused on big data, some of the challenges highlighted during the sessions included:What kind of data ecosystem is needed to share data across the federal government?How do we secure privacy of data?What are the key issues for big data analysis in the intelligence community?What can we do to prevent access to information to reduce insider threats?What opportunities does big data present for the federal government?What are the challenges facing agencies to adopt big data solutions?These are just some of the questions that panelist and audience members discussed during the Cloudera Federal Forum. The morning keynoteDavid R. Shedd  Deputy Director, Defense Intelligence Agency, provided some insights to the challenges facing government while adopting big data tools, and technology solutions. Shedd provided an interesting observation, as he noted, Bureaucracy will choose failure over change. The reason I say that is by and large, the bureaucratic construct where you work will look in the rear view mirror and say, were doing ok based on how we got here, rather than where we need to go.He continued to describe that what is critical is that agencies have champions and leadership driving agencies where they need to go. There were also several challenges he noted for adopting big data tools. Ive highlighted six of them below.1. Remove barriers to innovation:This means that organizations should continue to move towards supporting a culture of innovation, taking calculated risks and encourage staff to think out of the box and creatively.2. The acquisition process is arcane:The acquisition process moves too slow and far to complex. It does not allow government to swiftly procure tools to meet mission need. If there was a more rapid acquisition process, agencies could be more agile and flexible to leverage big data.3. Leaders understand the power of big data:More sharing and case studies are needed to help connect and teach leaders about the power of big data.4. Dynamic new world of threats:Simply, the threat landscape for government is changing. There are more threats facing government than ever before, and these threats are risking the economic viability of our nation. With these threats occurring, it is essential that organizations understand how to securely protect data in information systems.6. Finding the right data: creating the data ecosystem:Theres the saying, You dont know, what you dont know, and this couldnt be more true than with big data analysis. The reality is that since so much data is being created, stored and managed, across multiple departments and agencies, there are millions of insights to be learned. For government, a challenging question is: how do we get people the right information, at the right time, to make the right decision? This means that data must be shared broadly across an agency, while still retaining any privacy and security concerns.Six Lessons Learned for Big Data AdoptionYet, the event certainly was not all about the challenges facing government to adopt big data. During the morning panel government leaders discussed the best practices and lessons learned from their agency. The panel included:Moderated by LTG (R) Richard ZahnerJohn A. Marshall, Deputy Program Manager, NSG Program Management Office, IT Service Directorate, National Geospatial Intelligence AgencySkip McCormick, Systems Architect, Central Intelligence AgencyKevin Ford, Deputy Director, Technology Directorate, National Security AgencyTodd G. Myers  Lead Global Compute Architect  National GeospatialI looked back at my notes, and pulled out the six core best practices I heard from the panelist.1. Get buy-in by showing value:One strategy to get buy-in is to show the value that big data analysis will bring to your agency. Show core metrics and a clear problem that you will be solving by leveraging data. Conduct estimates and make it real and relevant for your agency.2. Governance is essential:Governance is essential to managing data. Your agency must make sure your policies are up-to-date. Also, be sure that you are enforcing your policy. You could have a great policy written, but without any kind of enforcement mechanism, it will fall flat.3. Tight budgets lead to innovation:I heard this a lot during the panel. Weve also seen this trend highlighted on GovLoop. With tight budgets, agency leaders must think even more critically how to be prudent users of limited funding.4. Define personas to manage data:This was an interesting observation from the panel. This strategy can be used to combat insider threats. Every employee will need different kinds of accessibility, make sure that your data system maps to these needs and is not providing unauthorized access.5. Start small, build framework and work out:When beginning any big data project, or any IT initiative, starting small and building out is a great way to test and iterate. You can spot roadblocks, understand challenges and prevent errors early. Ultimately, this leads to stronger products and services for stakeholders.6. Create communities of practice:Big data requires the sharing of knowledge, best practices and case studies. If you can create a community of practice within your agency, you can help to build support across your department, encourage knowledge sharing and start to build a data culture.The reality facing government is that big data is a trend that is moving fast, and many agencies are capitalizing on the opportunity that big data presents. With these new opportunities, it is essential that government keeps security paramount, and creates infrastructures that support agility, flexibility and security.Originally published by Pat Fiorenza on GovLoopThe rate of change in the Big Data landscape is astonishing. Most impressive is the degree to which the innovation that enables an enterprise data hub (EDH) tackles real challenges faced by business decision-makers every day. Where other enterprise technology actors have invested in a technical roadmap designed for the next bake-off, Clouderas customers have asked for more than just marginal performance improvements and feature upgrades. We work with our partners to push the ecosystem towards secular, permanent improvements, delivering new products that align to the enterprise capabilities that drive measurable business value at the top and bottom lines.An enterprise data strategy built around an EDH goes beyond delivering better throughput for your service-level agreements. It increases the availability and accessibility of data for the applications that support business growth, lays the foundation for new analytics projects that increase competitiveness and opportunity, and provides a full picture of your operations to enable process innovation. Moreover, an EDH requires up to 99% less capital expenditure per terabyte than a traditional data warehouse.A Next-Generation Business EngineAs we recently addressed some of the practical uses of an EDH, the enterprise data hub can alleviate the pain of slow data warehouse performance, accommodate huge and varied data sets for more meaningful and timely analytics, and enable real-time anomaly detection without duplicating data or building a custom solution.However, the greatest promise of the information-driven enterprise resides in the vertical-specific questions business leaders have historically been unable or afraid to ask, whether because of a lack of coherency in their data or the prohibitively high cost of specialized tools. The EDH encourages these new and bigger questions with an eye towards helping decision-makers bring the future of their industries to the present.EDH for Financial Services. Every sector of the global financial industry faces tremendous risk and regulatory need on a daily basis. When data is freed from silos, secured, and made available to the analysts that answer key questions about the marketas they need it, in its original form, and accessed via familiar toolseveryone in the C-suite can rest assured that the EDH provides a complete view of the business, perhaps for the first time.How do we use several decades worth of customer data to detect fraud without having to build out dedicated systems or limit our view to a small sample size?What does a 360-degree view of the customer across various distinct lines of business tell us about downstream opportunity and risk?EDH for Public Sector. The combination of data-at-rest and data-in-motionfrom sensors, surveillance, etc.for real-time analytics holds tremendous promise across branches of the government, but depends on multiple applications being able to process data on an ad hoc basis using the right tools at the right time. An EDH can manage interactions such that multiple users can query and explore data simultaneously at petabyte scale but with different access levels based on profile and permissions.Can we analyze video frame-by-frame in real time to detect threats and combine that with geospatial, signals intelligence, and other data to paint a complete security picture?What data would be required to predict catastrophic weather events, and how do we proactively capture, analyze, and store it at lower cost than the damage we intend to prevent?EDH for Telecommunications. Service providers are among the worlds biggest aggregators of consumer data and work under the most uncertain regulatory conditions. With an EDH, telecom companies can sync user, transaction, network, and service data across multiple platforms in a single place at considerably lower cost. Given rapid systems consolidation and the competition to acquire and retain business, you have the tools to research, develop, and bring new products to market in a fraction of the time.How do we monetize the terabytes of real-time geo-location, mobile interaction, content partner, Bluetooth, and external data we collect every day?Can we speed up our data processing pipelines by at least an order of magnitude to serve our customers and partners better and remain competitive?EDH for Healthcare. A predominance of legacy systems and the emergence of strict new regulations have added complexity to the health and pharma industries. While good information can drive early detection, improved patient outcomes, more efficient trials, and smarter capital outlay, siloed warehouses obscure the data required to answer the biggest questions. Rather than investing in additional bandwidth and expensive storage, healthcare and biotech companies have begun to land all their data in an EDH that serves as the core of their data center.How do we better prevent adverse effects using the massive trial data available on thousands of drugs, millions of compounds, and countless individual genetic variations?Can we personalize healthcare and minimize unnecessary ED visits by collecting and storing patient data from remote, wearable sensors in real time?EDH for Retail. Retailers have more diverse transactional data than ever, which holds opportunity for deeper customer insights when synchronized with online and offline behavior data, but also poses a threat from cyber-attackers that threaten privacy, denial of service, and theft. Moreover, data silos keep data scientists from identifying new business opportunities and prevent the just-in-time analyses that are required for fraud detection. The EDH centralizes all your data so that it is available not only for traditional reporting, but also for ad hoc projects and strategic initiatives that drive revenue.What can we do to optimize key metrics like same-store-sales and promotion efficacy by combining our online and offline data from social, surveillance, point-of-sale, marketing, etc.?Can we model predictive indicators that identify anomalies and help prevent the massive security attacks that cost us billions every year and dilute customer sentiment?The Information-Driven ExecutiveWithout the rich data that drives opportunity in the modern business context, decision-makers across every industry will continue to struggle with information paralysis and its high cost to the enterprise.With an EDH, enterprises can now take on more advanced workloads and realize new strategic benefits from their data. Cloudera strives to offer its customers the Big Data solution with the highest level of extensibility and security to enable the sophisticated recommendation systems, security information and event management, graph analytics, and machine learning capabilities that monetize data, without the costs typically associated with specialized tools.Read more about The Business Value of an Enterprise Data Hub Check out Cloudera Enterprise and download a trial of the Enterprise Data Hub Edition Amr Awadallah, chief technology officer, Cloudera keynotes at Strata 2014:Our legacy information architecture is not able to cope with the realities of todays business. This is because it is not able to scale to meet our SLAs due to separation of storage and compute, economically store the volumes and types of data we currently confront, provide the agility necessary for innovation, and most importantly, provide a full 360 degree view of our customers, products, and business. In this talk Dr. Amr Awadallah will present the Enterprise Data Hub (EDH) as the new foundation for the modern information architecture. Built with Apache Hadoop at the core, the EDH is an extremely scalable, flexible, and fault-tolerant, data processing system designed to put data at the center of your business.nbspEli Collins, chief technologist, Cloudera talks to OReilly Media about the Enterprise Data Hub and the Federal Government.What is it? Why is it important? What use cases can the Government use an enterprise data hub for?Big data is no longer a nascent concept. In the past few years, organizations have been using Hadoop, a file system with batch data processing framework, to solve many challenges brought on not only by the sheer volume of data, but also by the various types of data. However, as powerful and scalable as it was, Hadoop was not a technology by itself to address emerging business problems and opportunities created by the unprecedented growth in data. It lacked many critical capabilities, not the least of which is the ability to augment existing systems and leverage existing investments and skill sets.So during Hadoop World last year, our Chief Strategy Officer, Mike Olson blogged about our vision of a data management platform that is able to store any kind of data, in any volume, in full fidelity, for as long as you need it while providing robust security, including strong encryption, access control, logging, auditing and compliance reporting. In addition, this data management platform offers a rich  and, over time, growing  set of tools for processing and analyzing data, in place, and needs to connect to the database, data warehouse, document repositories and other systems that enterprises use to manage data.He dubbed it the enterprise data hub (EDH).But it is much more than a name. It is a philosophy in how we have envisioned, designed and built our products in the last five years, since the inception of Cloudera. It is a hub, designed to allow all the parts to come together, to carry out a bigger mission. We knew the EDH would not reach its full potential without working closely with a very comprehensive ecosystem of partners. So while we have led the market in product innovation, we also deliberately built a partner ecosystem that fulfills on a core attribute of an EDH  an open and extensible framework to enable customers maximum choice and value for their investment.The different categories of 800+ Cloudera Connect partners we work very closely with to build joint solutions are in a number of areas:Hardware, Cloud and reseller partners, enabling customers purchasing and deployment flexibilitySystems integration and services partners, to work with customers to build and deploy business solutions leveraging Cloudera as their EDHTraining partners, to deliver our world-class Cloudera University education broadly throughout the worldSoftware and OEM partners, to deliver applications and tools in association with Cloudera to drive better decision making and business outcomesTechnology partners, to connect and extend our systems and provide seamless operation in the modern IT environmentHardware, Cloud, and Reseller Partners for Purchasing and Deployment FlexibilityThe key to an information-driven enterprise is to minimize data movement. Together with partners, we offer our customers the ability to deploy an enterprise data hub where and how they need it: through hardware providers on premise, through hosting and MSP providers in the public cloud, through technology partners across private cloud environments, or combination of any of the above. Additionally, we have 60+ reseller partners globally, allowing customers to purchase Cloudera Enterprise however they want. The goal is to always bring storage and processing to the data.System Integration Partners to Build and Deploy SolutionsBecause an enterprise data hub is the heart and soul of becoming information-driven, implementing this next-generation data management platform requires specific skills and industry knowledge that are of high demand. Our SI partners simplify the approach to big data and guide customers with a clear methodology and solutions to achieve their goals. By working with any of our 500+ SI partners, customers are able to obtain services and to ramp up/down resources based on requirements at the time. Additionally, as the pioneer in developing and implementing the EDH, Cloudera is committed to educating and training our SI partners, and to share the best practice we have learned over the years.Training Partners to Get the Most Out of ClouderaCloudera University and dozens of global training partners are the leading providers of training and certification in data management and advanced analytics. These courses are not only built to provide job-specific and technology in-depth training, but also includes use cases from all verticals, including financial services, healthcare, digital media, advertising, bio-science, education and telecommunications.Software and OEM Partners to Drive Better Decision Making and Business OutcomesNo one company by itself can develop all the innovation that enterprises require. [We] allow our customers to tap into the best innovations the open source community has to offer, whether that innovation was developed by Cloudera or not, said Charles Zedlewski, vice president, products at Cloudera at the launch of Cloudera Connect: Innovators program. From getting data from silos into the EDH to running a wide variety of applications and tools against the Cloudera environment, we have over 200 software and OEM partners in the data integration, data security, master data management, BI, advanced analytics and applications space to help our customers derive the most value from all their data. Every product innovation and enhancement are designed with these software applications in mind, to realize the true data hub vision.Technology Partners to Provide Seamless Operation Among SystemsThe modern IT environment incorporates a number of hardware, software, networking, systems and components, all of which need to work together to deliver business value. Cloudera is committed to working with providers of these technologies to ensure interoperability, enabling customers choice in running the right workload in the right system and to get the best performance characteristics and value for their investment.ConclusionThe whole is greater than the sum of its parts. These famous words of the great philosopher, Aristotle, cannot be any more fitting to describe our vision of the enterprise data hub. And only with relationships with 800+ partners in the Cloudera Connect program, can we ensure our customers can deploy an enterprise data hub where and how they need to, are able to purchase Cloudera where and how they want to, can learn and train on Cloudera, can build and deploy solutions efficiently, and can run a wide variety of applications and tools against their Cloudera environment. And ultimately, ensure that they can be successful in becoming an information-driven enterprise.In the world of Data Warehousing, speed, cost and value are often of paramount importance. Consider the following:TDWI estimates it takes upwards of 8 weeks to add a column to a tableThe average cost of an integration project runs between $250K and $1M, according to GartnerOnly 3 of 10 customers surveyed by Ventana Research trust the data in their data warehouseDespite these constraints, the enterprise data warehouse (EDW) remains one of the most important tools at the center of any organization. Unfortunate as the dynamics of data rapidly change, the EDW is under constant pressure causing budgets to swell and confidence to wane.Often times this is avoidable. With the influx of new types and ever increasing volumes of data, conventional wisdom suggests that the EDW must be retrofitted in response. When the value of that data is not yet known, however, the effort and cost are likely something that could have been avoided. Instead organizations try to load this data anyway, integration processes start to run long (or over!), scarce EDW resources are used up to try shrink that time, reports and queries start to run longer than expected, and data is archived to try make room for rapidly growing data sets. In the end, the EDW becomes overburdened, can cost more and causes day-to-day business intelligence (BI) to suffer.As we announced last week, an enterprise data hub (EDH) based on Apache Hadoop is emerging as the leading solution for big data. In addition to storage and processing benefits, an EDH is ideal companion to an EDW that helps alleviate these challenges. When deployed along side common EDW infrastructure, an EDH helps streamline data integration processing, by offloading data transformations and improving performance, thus reducing costs. It also enables true self-service, exploratory BI  leveraging the tools you already have  reducing time to insight. Ultimately, the combination of an EDH and an EDW allow you to store all your data, in full fidelity and bring compute to the data without the need to move data around the enterprise.To help you get started, check out the Cloudera and Informatica reference architecture for data warehouse optimization. Well be talking about this topic and more at our upcoming Cloudera Sessions tour. Check out our events page to learn more.Understanding how checkpointing works in HDFS can make the difference between a healthy cluster or a failing one.Checkpointing is an essential part of maintaining and persisting filesystem metadata in HDFS. Its crucial for efficient NameNode recovery and restart, and is an important indicator of overall cluster health. However, checkpointing can also be a source of confusion for operators of Apache Hadoop clusters.In this post, Ill explain the purpose of checkpointing in HDFS, the technical details of how checkpointing works in different cluster configurations, and then finish with a set of operational concerns and important bug fixes concerning this feature.Filesystem Metadata in HDFSTo start, lets first cover how the NameNode persists filesystem metadata.HDFS metadata changes are persisted to the edit log.At a high level, the NameNodes primary responsibility is storing the HDFS namespace. This means things like the directory tree, file permissions, and the mapping of files to block IDs. Its important that this metadata (and all changes to it) are safely persisted to stable storage for fault tolerance.This filesystem metadata is stored in two different constructs: the fsimage and the edit log. The fsimage is a file that represents a point-in-time snapshot of the filesystems metadata. However, while the fsimage file format is very efficient to read, its unsuitable for making small incremental updates like renaming a single file. Thus, rather than writing a new fsimage every time the namespace is modified, the NameNode instead records the modifying operation in the edit log for durability. This way, if the NameNode crashes, it can restore its state by first loading the fsimage then replaying all the operations (also called edits or transactions) in the edit log to catch up to the most recent state of the namesystem. The edit log comprises a series of files, called edit log segments, that together represent all the namesystem modifications made since the creation of the fsimage.As an aside, this pattern of using a log for incremental changes on top of another storage format is quite common for traditional filesystems. Log-structured filesystems take this to an extreme and use just a log for persisting data, but more common journaling filesystems like EXT3, EXT4, and XFS support writing changes to a journal before applying them to their final locations on disk.Why is Checkpointing Important?A typical edit ranges from 10s to 100s of bytes, but over time enough edits can accumulate to become unwieldy. A couple of problems can arise from these large edit logs. In extreme cases, it can fill up all the available disk capacity on a node, but more subtly, a large edit log can substantially delay NameNode startup as the NameNode reapplies all the edits. This is where checkpointing comes in.Checkpointing is a process that takes an fsimage and edit log and compacts them into a new fsimage. This way, instead of replaying a potentially unbounded edit log, the NameNode can load the final in-memory state directly from the fsimage. This is a far more efficient operation and reduces NameNode startup time.Checkpointing creates a new fsimage from an old fsimage and edit log.However, creating a new fsimage is an I/O- and CPU-intensive operation, sometimes taking minutes to perform. During a checkpoint, the namesystem also needs to restrict concurrent access from other users. So, rather than pausing the active NameNode to perform a checkpoint, HDFS defers it to either the SecondaryNameNode or Standby NameNode, depending on whether NameNode high-availability is configured. The mechanics of checkpointing differs depending on if NameNode high-availability is configured; well cover both.In either case though, checkpointing is triggered by one of two conditions: if enough time has elapsed since the last checkpoint (dfs.namenode.checkpoint.period), or if enough new edit log transactions have accumulated (dfs.namenode.checkpoint.txns). The checkpointing node periodically checks if either of these conditions are met (dfs.namenode.checkpoint.check.period), and if so, kicks off the checkpointing process.Checkpointing with a Standby NameNodeCheckpointing is actually much simpler when dealing with an HA setup, so lets cover that first.When NameNode high-availability is configured, the active and standby NameNodes have a shared storage where edits are stored. Typically, this shared storage is an ensemble of three or more JournalNodes, but thats abstracted away from the checkpointing process.The standby NameNode maintains a relatively up-to-date version of the namespace by periodically replaying the new edits written to the shared edits directory by the active NameNode. As a result, checkpointing is as simple as checking if either of the two preconditions are met, saving the namespace to a new fsimage (roughly equivalent to running `hdfs dfsadmin -saveNamespace` on the command line), then transferring the new fsimage to the active namenode via HTTP.Checkpointing with NameNode HA configuredHere, Standby NameNode is abbreviated as SbNN and Active NameNode as ANN:SbNN checks whether either of the two preconditions are met: elapsed time since the last checkpoint or number of accumulated edits.SbNN saves its namespace to an a new fsimage with the intermediate name fsimage.ckpt_, where txid is the transaction ID of the most recent edit log transaction. Then, the SbNN writes an MD5 file for the fsimage, and renames the fsimage to fsimage_. While this is taking place, most other SbNN operations are blocked. This means administrative operations like NameNode failover or accessing parts of the SbNNs webui. Routine HDFS client operations (such as listing, reading, and writing files) are unaffected as these operations are serviced by the ANN.SbNN sends an HTTP GET to the active NNs GetImageServlet at /getimage?putimage=1. The URL parameters also have the transaction ID of the new fsimage and the SbNNs hostname and HTTP port.The active NNs servlet uses the information in the GET request to in turn do its own GET back to the SbNNs GetImageServlet. Similar to the standby, it first saves the new fsimage with the intermediate name fsimage.ckpt_, creates the MD5 file for the fsimage, and then renames the new fsimage to fsimage_.Checkpointing with a SecondaryNameNodeIn a non-HA deployment, checkpointing is done on the SecondaryNameNode rather than the standby NameNode. Since there isnt a shared edits directory or automatic tailing of the edit log, the SecondaryNameNode has to go through a few more steps first to refresh its view of the namespace before continuing down the same basic steps.Time diagram of 2NN and NN with annotated stepsHere, the NameNode is abbreviated as NN and the SecondaryNameNode as 2NN:2NN checks whether either of the two preconditions are met: elapsed time since the last checkpoint or number of accumulated edits.In the absence of a shared edit directory, the most recent edit log transaction ID needs to be queried via an explicit RPC to the NameNode (NamenodeProtocol#getTransactionId).2NN triggers an edit log roll, which ends the current edit log segment and starts a new one. The NN can keep writing edits to the new segment while the SNN compacts all the previous ones. This also returns the transaction IDs of the current fsimage and the edit log segment that was just rolled. Explicit triggering of an edit log roll is not necessary in an HA configuration, since the standby NameNode periodically rolls the edit log orthogonal to checkpointing.Given these two transaction IDs, the 2NN fetches new fsimage and edit files as needed via  GET to the NNs GetImageServlet. The 2NN might already have some of these files from a previous checkpoint (such as the current fsimage).If necessary, the 2NN reloads its namespace from a newly downloaded fsimage.The 2NN replays the new edit log segments to catch up to the current transaction ID.From here, the rest is the same as in the HA case with a StandbyNameNode.2NN writes out its namespace to a new fsimage.The 2NN contacts the NN via HTTP GET at /getimage?putimage=1, causing the NNs servlet to do its own  GET to the 2NN to download the new fsimage.Operational ImplicationsThe biggest operational concern related to checkpointing is when it fails to happen. Weve seen scenarios where NameNodes accumulated hundreds of GBs of edit logs, and no one noticed until the disks filled completely and crashed the NN. When this happens, theres not much to do besides restart the NN and wait for it to replay all the edits. Because of the potential severity of this issue, Cloudera Manager will warn if the current fsimage is out of date, if the checkpointing 2NN or SbNN is down, as well as if NN disks are close to capacity.Checkpointing is a very I/O and network intensive operation and can affect client performance. This is especially true on a large cluster with millions of files and a multi-GB fsimage, since copying a new fsimage to the NameNode can eat up all available bandwidth. In this case, the transfer speed can be throttled with dfs.image.transfer.bandwidthPerSec. If you do adjust this parameter, you might also need to adjust dfs.image.transfer.timeout based on your expected transfer time.If youre on an older version of CDH, there are also a number of issues related to checkpointing that might make upgrading worthwhile.HDFS-4304 (fixed in CDH 4.1.4, 4.2.1, and 4.3.0). Previously, it was possible to write an edit log operation so big that it couldnt be read when replaying the edit log. This would cause checkpointing and NameNode startup to fail. This was a problem for files with lots of blocks, since closing a file involved writing all the block IDs to the edit log. The fix was to simply increase the size of the maximum allowable edit log operation.HDFS-4305(fixed in CDH 4.3.0).Related to HDFS-4304 above, files with a large number of blocks are typically due to misconfiguration. For example, a user might accidentally set a block size of 128KB rather than 128MB, or might only use a single reducer for a large MapReduce job. This issue was fixed by having the NameNode enforce a minimum block size as well as a maximum number of blocks per file.HDFS-4816 (fixed in CDH 4.5.0). Previously, image transfer from the standby NN to the active NN held the standby NNs write lock. Thus if the active NN failed during image transfer, the standby NN would not be able to failover until the transfer completed. Since transferring the fsimage doesnt actually modify any namespace data, the transfer was simply moved outside the critical section.HDFS-4128 (fixed in CDH 4.3.0). If the 2NN hit an out-of-memory (OOM) exception during edit log replay, it could get stuck in an inconsistent state where it would try replaying the edit log from the incorrect offset during future checkpointing attempts. Since its likely that this OOM would keep happening even if we fixed log replay, the 2NN now simply aborts if it fails to replay logs a few times. The underlying fix though is to configure your 2NN with the same heap size as your NameNode.HDFS-4300 (fixed in CDH 4.3.0). If the 2NN or SbNN experienced an error while transferring an edits file, it would not retry downloading the complete file later. This process would stall checkpointing, since itd be impossible to replay the partial edits file. This issue was fixed by first transferring the edits file to a temporary location and then renaming it to its final destination after transfer completes.HDFS-4569 (fixed in CDH4.2.1 and CDH4.3.0). Bumped the image transfer timeout from 1 minute to 10 minutes. The default timeout was causing issues for checkpointing with multi-GB fsimages, especially with throttling turned on.ConclusionCheckpointing is a vital part of healthy HDFS operation. In this post, you learned how filesystem metadata is persisted in HDFS, the importance of checkpointing to this role, how checkpointing works in both HA and non-HA setups, and finally covered a selection of important fixes and improvements related to checkpointing.By understanding the purpose of checkpointing and how it works, you are now equipped with the knowledge to debug these kinds of issues in your production clusters.Andrew Wang is a Software Engineer at Cloudera and a Hadoop committer.Spark is a compelling multi-purpose platform for use cases that span investigative, as well as operational, analytics.Data science is a broad church. I am a data scientist  or so Ive been told  but what I do is actually quite different from what other data scientists do. For example, there are those practicing investigative analytics and those implementing operational analytics. (Im in the second camp.)Data scientists performing investigative analytics use interactive statistical environments like R to perform ad-hoc, exploratory analytics in order to answer questions and gain insights. By contrast, data scientists building operational analytics systems have more in common with engineers. They build software that creates and queries machine-learning models that operate at scale in real-time serving environments, using systems languages like C++ and Java, and often use several elements of an enterprise data hub, including the Apache Hadoop ecosystem.And there are subgroups within these groups of data scientists. For example, some analysts who are proficient with R have never heard of Python or scikit-learn, or vice versa, even though both provide libraries of statistical functions that are accessible from a REPL (Read-Evaluate-Print Loop) environment.A World of TradeoffsIt would be wonderful to have one tool for everyone, and one architecture and language for investigative as well as operational analytics. If I primarily work in Java, should I really need to know a language like Python or R in order to be effective at exploring data? Coming from a conventional data analyst background, must I understand MapReduce in order to scale up computations? The array of tools available to data scientists tells a story of unfortunate tradeoffs:R offers a rich environment for statistical analysis and machine learning, but it has some rough edges when performing many of the data processing and cleanup tasks that are required before the real analysis work can begin. As a language, its not similar to the mainstream languages developers know.Python is a general purpose programming language with excellent libraries for data analysis like Pandas and scikit-learn. But like R, its still limited to working with an amount of data that can fit on one machine.Its possible to develop distributed machine learning algorithms on the classic MapReduce computation framework in Hadoop (see Apache Mahout). But MapReduce is notoriously low-level and difficult to express complex computations in.Apache Crunch offers a simpler, idiomatic Java API for expressing MapReduce computations. But still, the nature of MapReduce makes it inefficient for iterative computations, and most machine learning algorithms have an iterative component.And so on. There are both gaps and overlaps between these and other data science tools. Coming from a background in Java and Hadoop, I do wonder with envy sometimes: why cant we have a nice REPL-like investigative analytics environment like the Python and R users have? Thats still scalable and distributed? And has the nice distributed-collection design of Crunch? And can equally be used in operational contexts?Common Ground in SparkThese are the desires that make me excited about Apache Spark. While discussion about Spark for data science has mostly noted its ability to keep data resident in memory, which can speed up iterative machine learning workloads compared to MapReduce, this is perhaps not even the big news, not to me. It does not solve every problem for everyone. However, Spark has a number of features that make it a compelling crossover platform for investigative as well as operational analytics:Spark comes with a machine-learning library, MLlib, albeit bare bones so far.Being Scala-based, Spark embeds in any JVM-based operational system, but can also be used interactively in a REPL in a way that will feel familiar to R and Python users.For Java programmers, Scala still presents a learning curve. But at least, any Java library can be used from within Scala.Sparks RDD (Resilient Distributed Dataset) abstraction resembles Crunchs PCollection, which has proved a useful abstraction in Hadoop that will already be familiar to Crunch developers. (Crunch can even be used on top of Spark.)Spark imitates Scalas collections API and functional style, which is a boon to Java and Scala developers, but also somewhat familiar to developers coming from Python. Scala is also a compelling choice for statistical computing.Spark itself, and Scala underneath it, are not specific to machine learning. They provide APIs supporting related tasks, like data access, ETL, and integration. As with Python, the entire data science pipeline can be implemented within this paradigm, not just the model fitting and analysis.Code that is implemented in the REPL environment can be used mostly as-is in an operational context.Data operations are transparently distributed across the cluster, even as you type.Spark, and MLlib in particular, still has a lot of growing to do. For example, the project needs optimizations, fixes, and deeper integration with YARN. It doesnt yet provide nearly the depth of library functions that conventional data analysis tools do. But as a best-of-most-worlds platform, it is already sufficiently interesting for a data scientist of any denomination to look at seriously.In Action: Tagging Stack Overflow QuestionsA complete example will give a sense of using Spark as an environment for transforming data and building models on Hadoop. The following example uses a dump of data from the popular Stack Overflow QA site. On Stack Overflow, developers can ask and answer questions about software. Questions can be tagged with short strings like java or sql. This example will build a model that can suggest new tags to questions based on existing tags, using the alternating least squares (ALS) recommender algorithm; questions are users and tags are items.Getting the DataStack Exchange provides complete dumps of all data, most recently from January 20, 2014. The data is provided as a torrent containing different types of data from Stack Overflow and many sister sites. Only the file stackoverflow.com-Posts.7z needs to be downloaded from the torrent.This file is just a bzip-compressed file. Spark, like Hadoop, can directly read and split some compressed files, but in this case it is necessary to uncompress a copy on to HDFS. In one step, thats:bzcat stackoverflow.com-Posts.7z | hdfs dfs -put - /user/srowen/Posts.xmlUncompressed, it consumes about 24.4GB, and contains about 18 million posts, of which 2.1 million are questions. These questions have about 9.3 million tags from approximately 34,000 unique tags.Set Up SparkGiven that Sparks integration with Hadoop is relatively new, it can be time-consuming to get it working manually. Fortunately, CDH hides that complexity by integrating Spark and managing setup of its processes. Spark can be installed separately with CDH 4.6.0, and is included in CDH 5 Beta 2. This example uses an installation of CDH 5 Beta 2.This example uses MLlib, which uses the jblas library for linear algebra, which in turn calls native code using LAPACK and Fortran. At the moment, it is necessary to manually install the Fortran library dependency to enable this. The package is called libgfortran or libgfortran3, and should be available from the standard package manager of major Linux distributions. For example, for RHEL 6, install it with:sudo yum install libgfortranThis must be installed on all machines that have been designated as Spark workers.Log in to the machine designated as the Spark master with ssh. It will be necessary, at the moment, to ask Spark to let its workers use a large amount of memory. The code in MLlib that is used in this example, in version 0.9.0, has a memory issue, one that is already fixed for the next release. To configure for more memory and launch the shell:export SPARK_JAVA_OPTS=-Dspark.executor.memory=8gspark-shellInteractive Processing in the ShellThe shell is the Scala REPL. Its possible to execute lines of code, define methods, and in general access any Scala or Spark functionality in this environment, one line at a time. You can paste the following steps into the REPL, one by one.First, get a handle on the Posts.xml file:val postsXML = sc.textFile(hdfs:///user/srowen/Posts.xml)In response the REPL will print:postsXML: org.apache.spark.rdd.RDD[String] = MappedRDD[1] at textFile at :12The text file is an RDD (Resilient Distributed Dataset) of Strings, which are the lines of the file. You can query it by calling methods of the RDD class. For example, to count the lines:postsXML.countThis command yields a great deal of output from Spark as it counts lines in a distributed way, and finally prints 18066983.The next snippet transforms the lines of the XML file into a collection of (questionID,tag) tuples. This demonstrates Scalas functional programming style, and other quirks. (Explaining them is out of scope here.) RDDs behave like Scala collections, and expose many of the same methods, like map:val postIDTags = postsXML.flatMap { line =  // Matches Id=... ... Tags=... in  line  val idTagRegex = Id=\(\d+)\.+Tags=\([^\]+)\.r  // // Finds tags like TAG value from above  val tagRegex = lt;([^]+)gt;.r  // Yields 0 or 1 matches:  idTagRegex.findFirstMatchIn(line) match {    // No match -- not a  line    case None = None    // Match, and can extract ID and tags from m    case Some(m) = {      val postID = m.group(1).toInt      val tagsString = m.group(2)      // Pick out just TAG matching group      val tags = tagRegex.findAllMatchIn(tagsString).map(_.group(1)).toList      // Keep only question with at least 4 tags, and map to (post,tag) tuples      if (tags.size = 4) tags.map((postID,_)) else None     }  }  // Because of flatMap, individual lists will concatenate  // into one collection of tuples}You will notice that this returns immediately, unlike previously. So far, nothing requires Spark to actually perform this transformation. It is possible to force Spark to perform the computation by, for example, calling a method like count. Or Spark can be told to compute and persist the result through checkpointing, for example.The MLlib implementation of ALS operates on numeric IDs, not strings. The tags (items) in this data set are strings. It will be sufficient here to hash tags to a nonnegative integer value, use the integer values for the computation, and then use a reverse mapping to translate back to tag strings later. Here, a hash function is defined since it will be reused shortly.def nnHash(tag: String) = tag.hashCode  0x7FFFFFvar tagHashes = postIDTags.map(_._2).distinct.map(tag =(nnHash(tag),tag))Now, you can convert the tuples from before into the format that the ALS implementation expects, and the model can be computed:import org.apache.spark.mllib.recommendation._// Convert to Rating(Int,Int,Double) objectsval alsInput = postIDTags.map(t = Rating(t._1, nnHash(t._2), 1.0))// Train model with 40 features, 10 iterations of ALSval model = ALS.trainImplicit(alsInput, 40, 10)This will take minutes or more, depending on the size of your cluster, and will spew a large amount of output from the workers. Take a moment to find the Spark master web UI, which can be found from Cloudera Manager, and will run by default at http://[master]:18080. There will be one running application. Click through, then click Application Detail UI. In this view its possible to monitor Sparks distributed execution of lines of code in ALS.scala:When it is complete, a factored matrix model is available in Spark. It can be used to predict question-tag associations by recommending tags to questions. At this early stage of MLlibs life, there is not even a proper recommend method yet, that would give suggested tags for a question. However it is easy to define one:def recommend(questionID: Int, howMany: Int = 5): Array[(String, Double)] = {  // Build list of one question and all items and predict value for all of them  val predictions = model.predict(tagHashes.map(t = (questionID,t._1)))  // Get top howMany recommendations ordered by prediction value  val topN = predictions.top(howMany)(Ordering.by[Rating,Double](_.rating))  // Translate back to tags from IDs  topN.map(r = (tagHashes.lookup(r.product)(0), r.rating))}And to call it, pick any question with at least four tags, like How to make substring-matching query work fast on a large table? and get its ID from the URL. Here, thats 7122697:recommend(7122697).foreach(println)This method will take a minute or more to complete, which is slow. The lookups in the last line are quite expensive since each requires a distributed search. It would be somewhat faster if this mapping were available in memory. Its possible to tell Spark to do this:tagHashes = tagHashes.cacheBecause of the magic of Scala closures, this does in fact affect the object used inside the recommend method just defined. Run the method call again and it will return faster. The result in both cases will be something similar to the following:(sql,0.17745152481166354)(database,0.13526622226672633)(oracle,0.1079428707621154)(ruby-on-rails,0.06067207312463499)(postgresql,0.050933613169706474)(Your result will not be identical, since ALS starts from a random solution and iterates.) The original question was tagged postgresql, query-optimization, substring, and text-search. Its reasonable that the question might also be tagged sql and database. oracle makes sense in the context of questions about optimization and text search, and ruby-on-rails often comes up with PostgreSQL, even though these tags are not in fact related to this particular question.Something for EveryoneOf course, this example could be more efficient and more general. But for the practicing data scientists out there  whether you came in as an R analyst, Python hacker, or Hadoop developer  hopefully you saw something familiar in different elements of the example, and have discovered a way to use Spark to access some benefits that the other tribes take for granted.Learn more about Spark and its availability on CDH, and join the discussion in our brand-new Spark forum.Sean is Director of Data Science forEMEAat Cloudera, helping customers build large-scale machine learning solutions on Hadoop. Previously, Sean founded Myrrix Ltd, producing a real-time recommender and clustering product evolved from Apache Mahout. Sean was primary author of recommender components in Mahout, and has been an active committer andPMCmember for the project. He is co-author of Mahout in Action.Hue users can learn a lot about new features by following a steady stream of new demos.Hue, the open source Web UI that makes Apache Hadoop easier to use, is now a standard across the ecosystem  shipping within multiple software distributions and sandboxes. One of the reasons for its success is an agile developer community behind it that is constantly rolling out new features to its users.Just as important, the Hue team is diligent in its documentation and demonstration of those new features via video demos. In this post, for your convenience, I bring you the most recent examples (released since December): The new Spark Igniter App Using YARN and Job BrowserJob Browser with YARN SecurityApache Oozie crontab schedulingYou can stay up to date about new demos by following the Hue teams Vimeo channel and/or the Hue blog. However, I will bring you similar updates here from time to time.Justin Kestelyn is Clouderas developer outreach director.Clouderas own enterprise data hub is yielding great results for providing world-class customer support.Here at Cloudera, we are constantly pushing the envelope to give our customers world-class support. One of the cornerstones of this effort is the Cloudera Support Interface (CSI), which weve described in prior blog posts (here and here). Through CSI, our support team is able to quickly reason about a customers environment, search for information related to a case currently being worked, and much more.In this post, Im happy to write about a new feature in CSI, which we call Monocle Stack Trace.Stack Trace Exploration with SearchHadoop log messages and the stack traces in those logs are critical information in many of the support cases Cloudera handles. We find that our customer operation engineers (COEs) will regularly search for stack traces they find referenced in support cases to try to determine where else that stack trace has shown up, and in what context it would occur. This could be in the many sources we were already indexing as part of Monocle Search in CSI: Apache JIRAs, Apache mailing lists, internal Cloudera JIRAs, internal Cloudera mailing lists, support cases, Knowledge Base articles, Cloudera Community Forums, and the customer diagnostic bundles we get from Cloudera Manager.It turns out that doing routine document searches for stack traces doesnt always yield the best results. Stack traces are relatively long compared to normal search terms, so search indexes wont always return the relevant results in the order you would expect. Its also hard for a user to churn through the search results to figure out if the stack trace was actually an exact match in the document to figure out how relevant it actually is.To solve this problem, we took an approach similar to what Google does when it wants to allow searching over a type that isnt best suited for normal document search (such as images): we created an independent index and search result page for stack-trace searches. In Monocle Stack Trace, the search results show a list of unique stack traces grouped with every source of data in which unique stack trace was discovered. Each source can be viewed in-line in the search result page, or the user can go to it directly by following a link.We also give visual hints as to how the stack trace for which the user searched differs from the stack traces that show up in the search results. A green highlighted line in a search result indicates a matching call stack line. Yellow indicates a call stack line that only differs in line number, something that may indicate the same stack trace on a different version of the source code. A screenshot showing the grouping of sources and visual highlighting is below:The high-level implementation details are as follows:Every data source we fetch as part of standard Monocle Search indexing is marked for stack-trace processing.Every hour, a series of MapReduce jobs run to find and extract stack traces from the sources weve fetched.For each stack trace found, we create a unique key using the call stack that we use to uniquely identify an exception, and do a lookup in Apache HBase using that key. If theres already a row, we append the source we found the new stack trace in to that row. If not, we create a new row in HBase, and insert the new call stack into the Search index.When a search is executed, each unique stack trace that Solr sees as a match in the indexed is returned.For each stack trace returned, we then do a lookup against HBase to find the sources that stack trace has been found in.The UI then does a line by line comparison of the call stack, highlighting each call stack line in each search result appropriately.Ive left out some details about optimizations weve made, such as our use of HBases bulk loading functionality for the extraction of stack traces from some of our larger data sources, but those arent critical to understanding the high-level data flow.On the initial day of launch, we received feedback that the Monocle Stack Trace search index was especially useful when a COE was presented with a stack trace they know theyve seen in a prior support case before. This way of grouping and visualizing the matches was bringing our supporters right to the case that was being recalled, instead of them having to sift through the long list of fuzzy search results a regular document search would yield. This resulted in less time searching, and more time focusing on solving the problem.ConclusionWe feel like this application shows the power of an enterprise data hub (EDH). By having multiple strategies for storing, accessing, and processing data within our EDH, you can truly execute on building innovative applications that solve problems in new ways.This application goes way beyond simple indexing and searching. We are using Cloudera Search, HBase, and MapReduce to process, store, and visualize stack traces that wouldnt be possible with just a search index. How Monocle Stack Trace integrates with the larger CSI application goes way beyond that, though. Its a great feeling when you are able to execute a search in Monocle Stack Trace that links directly to a point in time in a customer log file that an Impala query returned after churning through tens of GBs of data  done interactively from a Web UI on the order of a second or two. At Cloudera, we strongly believe in investing in these kinds of applications in the name of giving our COEs that extra edge to provide world-class support.Adam Warrington is an Engineer Manager on the customer operations team at Cloudera.Hadoop 2.3.0 includes hundreds of new fixes and features, but none more important than HDFS caching.The Apache Hadoop community has voted to release Hadoop 2.3.0, which includes (among many other things):In-memory caching for HDFS, including centralized administration and managementGroundwork for future support of heterogeneous storage in HDFSSimplified distribution of MapReduce binaries via the YARN Distributed CacheYou can read the release notes here. Congratulations to everyone who contributed!As noted above, one of the major new features in Hadoop 2.3.0 is HDFS caching, which enables memory-speed reads in HDFS. This feature was developed by two engineers/Hadoop committers at Cloudera: Andrew Wang and Colin McCabe.HDFS caching lets users explicitly cache certain files or directories in HDFS. DataNodes will then cache the corresponding blocks in off-heap memory through the use of mmap and mlock. Once cached, Hadoop applications can query the locations of cached blocks and place their tasks for memory-locality. Finally, when memory-local, applications can use the new zero-copy read API to read cached data with no additional overhead. Preliminary benchmarks show that optimized applications can achieve read throughput on the order of gigabytes per second.Better yet, this feature will be landing in CDH 5.0 (which is based on Hadoop 2.3.0) when it ships alongside corresponding Impala improvements that take advantage of these new APIs for improved performance. So, you can look forward to an even faster Impala in the new release!Justin Kestelyn is Clouderas developer outreach director.Learn how to use Cloudera Search along with RBL-JE to search and index documents in multiple languages.Our thanks to Basis Technology for providing the how-to below!Basis Technologys Rosette Base Linguistics for Java (RBL-JE) provides a comprehensive multilingual text analytics platform for improving search precision and recall. RBL provides tokenization, lemmatization, POS tagging, and de-compounding for Asian, European, Nordic, and Middle Eastern languages, and has just been certified for use with Cloudera Search. Cloudera Search brings full-text, interactive search, and scalable indexing to Apache Hadoop by marrying SolrCloud with HDFS and Apache HBase, and other projects in CDH. Because its integrated with CDH, Cloudera Search brings the same fault tolerance, scale, visibility, and flexibility of your other Hadoop workloads to search, and allows for a number of indexing, access control, and manageability options.In this post, youll learn how to use Cloudera Search and RBL-JE to index and search documents. Since Cloudera takes care of the plumbing for distributed search and indexing, the only work needed to incorporate Basis Technologys linguistics is loading the software and configuring your Solr collections.First, install RBL-JE. This essentially involves unpacking a tar.gz file and copying your license file to the licenses directory. Note the root directory of the installation. Well refer to this as RBLJE_ROOT later.Searching and indexing with RBL-JE requires a few additions to the schema.xml and solr.xml files for each Solr collections that you will use. To the solrconfig.xml file, you will add these lines to ensure that the appropriate RBL jar files end up on the class path:lib path=[[RBLJE_ROOT]]/rbl-je-[[RBLJE_VER]]/lib/btrbl-je-[[RBLJE_VER]].jar /lib path=[[RBLJE_ROOT]]/rbl-je-[[RBLJE_VER]]/lib/btcommon-[[BT_COMMON_VER]].jar /lib path=[[RBLJE_ROOT]]/rbl-je-[[RBLJE_VER]]/lib/slf4j-api-[[SLF4J_VER]].jar /lib path=[[RBLJE_ROOT]]/rbl-je-[[RBLJE_VER]]/lib/slf4j-simple-[[SLF4J_VER]].jar /lib path=[[RBLJE_ROOT]]/rbl-je-[[RBLJE_VER]]/lib/btrbl-je-lucene-solr-[[LUCENE_SOLR_VER]]-[[RBLJE_VER]].jar /Replace the [[xxx]] text in the pathnames above to match the version of RBL you are using. The version numbers can be determined by looking at the contents your RBLJE_ROOT.Edit the schema.xml file to add field types that use RBL and assign them to fields in your documents. Here is an example field type that specifies using RBL to analyze Chinese data:fieldtype name=chinese-basis class=solr.TextField            analyzer                tokenizer class=com.basistech.rosette.lucene.BaseLinguisticsTokenizerFactory                           language=zhs                           licensePath=[[bt.license.path]]                           modelDirectory=[[bt.model.directory]]                        /                                filter class=com.basistech.rosette.lucene.BaseLinguisticsTokenFilterFactory                        language=zhs                        licensePath=[[bt.license.path]]                        dictionaryDirectory=[[bt.dictionary.directory]]                        addLemmaTokens=true/            /analyzer        /fieldtypeWhere:bt.license.path is [[RBLJE_ROOT]]/rbl-je-[[RBLJE_VER]]/licenses/rlp-license.xmlbt.model.directory is [[RBLJE_ROOT]]/rbl-je-[[RBLJE_VER]]/modelsbt.dictionary.directory is [[RBLJE_ROOT]]/rbl-je-[[RBLJE_VER]]/dictsAnd here is an example of using it on a field:field name=text type=chinese-basis indexed=true stored=true /Thats it! Once this bit of configuration is done, the Cloudera Search framework can be used conventionally for indexing and searching. Youll find a repository of configuration files, scripts, and sample documents that you can use to configure and test RBL-JE here. It provides working examples of the configuration techniques discussed above.Request an evaluation version of RBL-JE from Basis Technology and try it out in your own Cloudera Search application!Bringing Parquet support to Hive was a community effort that deserves congratulations!Previously, this blog introduced Parquet, an efficient ecosystem-wide columnar storage format for Apache Hadoop. As discussed in that blog post, Parquet encodes data extremely efficiently and as described in Googles original Dremel paper. (For more technical details on the Parquet format read Dremel made simple with Parquet, or go directly to the open and community-driven Parquet Format specification.)Before discussing the Parquet Hive integration, its worth discussing how widely Parquet has been adopted across the Hadoop ecosystem. Parquet integrates with the following engines:Cloudera ImpalaApache CrunchApache DrillApache Hadoop MapReduceApache Hive (0.10, 0.11, 0.12, and 0.13)Apache PigApache SparkApache Tajo (planned)Cascadingand the following data description software:Apache AvroApache ThriftGoogle Protocol Buffers (in code review)When Parquet was announced, Criteo stepped up to create the Parquet Hive integration. Initially this integration was hosted within the Parquet project and shipped with CDH 4.5. However, as the momentum behind Parquet grew, users wanted to use Parquet with a variety of Hive versions. Therefore, the Parquet team determined that native integration with the Hive project would be easier to maintain, as Hive does not have well defined public/private APIs. Furthermore, as can be seen below, native integration greatly simplifies the CREATE TABLE command.As such, the Parquet team decided to move the Parquet Hive integration into the Hive project via HIVE-5783. A diverse set of Parquet and Hive contributors came together to commit native Parquet support to Hive 0.13. Most notably, Criteo engineers Justin Coffey, Mickal Lacour, and Remy Pecqueur donated the Hive Parquet integration to the Hive project.The end result of this work is that users of Hive 0.13 and CDH 5 can easily create Parquet tables in Hive:CREATE TABLE parquet_test ( id int, str string, mp MAPSTRING,STRING, lst ARRAYSTRING, struct STRUCTA:STRING,B:STRING)PARTITIONED BY (part string)STORED AS PARQUET;Users of CDH 4.5 and Hive 0.10, 0.11, and 0.12 can continue to use Parquet Hive from the Parquet project proper, by using the older more verbose CREATE TABLE syntax. To create a table in Hive 0.10, 0.11, or 0.12, use the syntax below:CREATE TABLE parquet_test ( id int, str string, mp MAPSTRING,STRING, lst ARRAYSTRING, strct STRUCTA:STRING,B:STRING)PARTITIONED BY (part string)ROW FORMAT SERDE 'parquet.hive.serde.ParquetHiveSerDe'STORED ASINPUTFORMAT 'parquet.hive.DeprecatedParquetInputFormat'OUTPUTFORMAT 'parquet.hive.DeprecatedParquetOutputFormat';Thanks to everyone who contributed to this work!Brock Noland is a Software Engineer at Cloudera and a Hive Committer.Integrating Hue with LDAP can help make your secure Hadoop apps as widely consumed as possible. Hue, the open source Web UI that makes Apache Hadoop easier to use, easily integrates with your corporations existing identity management systems and provides authentication mechanisms for SSO providers. So, by changing a few configuration parameters, your employees can start analyzing Big Data in their own browsers under an existing security policy.In this blog post, youll learn details about the various features and capabilities available in Hue for integrating with likely the most popular authentication mechanism, LDAP. (It is also possible to authenticate Hue users via PAM, SPNEGO, OpenID, OAuth, and SAML, but those topics are for another post.)AuthenticationThe typical authentication scheme for Hue takes the following form:In the above diagram, credentials are validated against the Hue database. Often its easier to manage identities from a central location; with the Hue LDAP integration, users can use their LDAP credentials to authenticate and inherit their existing groups transparently. There is no need to save or duplicate any employee password in Hue:When authenticating via LDAP, Hue validates login credentials against a directory service if configured with this authentication backend:[desktop][[auth]]backend=desktop.auth.backend.LdapBackendThe LDAP authentication backend will automatically create users that dont exist in Hue by default. Hue needs to import users in order to properly perform the authentication. (The password is never imported when importing users.) However, you may want to disable automatic import at times to allow logins only by a predefined list of manually imported users. For those cases, you can use the following configuration to disable automatic import:[desktop][[ldap]]create_users_on_login=falseThe case sensitivity of the authentication process is defined in the Case Sensitivity section below.There are two different ways to authenticate with a directory service through Hue:Search BindThe search-bind mechanism for authenticating will perform an ldapsearch against the directory service and bind using the found distinguished name (DN) and password provided. This is, by default, used when authenticating with LDAP. The configurations that affect this mechanism are outlined in the LDAP Search section below.Direct BindThe direct-bind mechanism for authenticating will bind to the LDAP server using the username and password provided at login. You can choose between two options for how Hue binds:nt_domain  Domain component for User Principal Names (UPN) in active directory. This Active Directory-specific idiom allows Hue to authenticate with Active Directory without having to follow LDAP references to other partitions. This typically maps to the email address of the user or the users ID in conjunction with the domain.ldap_username_pattern  Provides a template for the DN that will ultimately be sent to the directory service when authenticating.If nt_domain is provided, Hue will use a UPN to bind to the LDAP service:[desktop][[ldap]]nt_domain=example.comOtherwise, the ldap_username_pattern configuration is used. (The  parameter will be replaced with the username provided at login):[desktop][[ldap]]ldap_username_pattern=uid=username,ou=People,DC=hue-search,DC=ent,DC=cloudera,DC=comTypical attributes to search for include:uidsAMAccountNameTo enable direct bind authentication, the search_bind_authentication configuration must be set to false:[desktop][[ldap]]search_bind_authentication=falseImporting UsersIf an LDAP user must belong to a certain group and have a particular set of permissions, you can import this user via the Useradmin interface:As you can see above, there are two options available when importing:Distinguished name  If this option is checked, the username provided must be a full distinguished name (for example: uid=hue,ou=People,dc=gethue,dc=com). Otherwise, the Username provided should be a fragment of a Relative Distinguished Name (rDN). (For example, the username hue maps to the rDN uid=hue.) Hue will perform an LDAP search using the same methods and configurations as defined in the LDAP Search section; essentially, Hue will take the provided username and create a search filter using the user_filter and user_name_attr configurations. Create home directory  If this option is checked, when the user is imported and their home directory in HDFS will automatically be created, if it doesnt already exist.The case sensitivity of the search and import processes are defined in the Case Sensitivity section.Importing GroupsGroups are importable via the Useradmin interface. Then, you can add users to this group, which would provide a set of permissions (such as accessing the Impala application). This function works similarly to user importing, but has a couple of extra features.As the above image portrays, not only can groups be discovered via DN and rDN search, but users that are members of the group and members of the groups subordinate groups can be imported as well. Posix groups and members are automatically imported if the group found has the object class posixGroup. Synchronizing Users and GroupsUsers and groups can be synchronized with the directory service via the Useradmin interface or via a command-line utility. The images from the previous sections use the words Sync to indicate that when a name of a user or group that exists in Hue is added, it will actually be synchronized instead. In the case of importing users for a particular group, new users will be imported and existing users will be synchronized. (Note: Users who have been deleted from the directory service will not be deleted from Hue. You can manually deactivate those users from Hue via the Useradmin interface.)Attributes synchronizedCurrently, only the first name, last name, and email address are synchronized. Hue looks for the LDAP attributes givenName, sn, and mail when synchronizing. Also, the user_name_attr config is used to appropriately choose the username in Hue. For example, if user_name_attr is set to uid, then the uid returned by the directory service will be used as the username of the user in Hue.Useradmin interfaceThe Sync LDAP users/groups button in the Useradmin interface will automatically synchronize all users and groups.Command-line interfaceHeres a quick example of how to use the command line interface to synchronize users and groups:hue root/build/env/bin/hue sync_ldap_users_and_groupsLDAP SearchThere are two configurations for restricting the search process:user_filter  General LDAP filter to restrict the searchuser_name_attr  The attribute that will be considered the username against which to searchHere is an example configuration:[desktop][[ldap]][[[users]]]user_filter=objectClass=*user_name_attr=uidWith the above configuration, the LDAP search filter will take the form:((objectClass=*)(uid=user entered usename))Case SensitivityYou can configure Hue to ignore the case of usernames as well as force usernames to lower case via the ignore_username_case and force_username_lowercase configurations. These two configurations should be used in conjunction with each other. This is useful when integrating with a directory service containing usernames in capital letters and UNIX usernames in lowercase letters (which is a Hadoop requirement). Here is an example of configuring them:[desktop][[ldap]]ignore_username_case=trueforce_username_lowercase=trueLDAPS/StartTLS supportSecure communication with LDAP is provided via the SSL/TLS and StartTLS protocols. It allows Hue to validate the directory service to which its going to converse. Practically speaking, if a Certificate Authority Certificate file is provided, Hue will communicate via LDAPS:[desktop][[ldap]]ldap_cert=/etc/hue/ca.crtThe StartTLS protocol can be used as well (step up to SSL/TLS):[desktop][[ldap]]use_start_tls=trueConclusionThe Hue team is working hard to improve security. Upcoming LDAP features include: Import nested LDAP groups and multi-domain support for Active Directory. We hope this brief overview of LDAP in Hue will help you make your system more secure, more compliant with current security standards, and open up big data analysis to many more users!As always, feel free to contact us at hue-user@ or @gethue!Abe Elmahrek is a Software Engineer at Cloudera.Thanks to the improvements described here, CDH 5 will ship with a version of MapReduce 2 that is just as fast (or faster) than MapReduce 1.Performance fixes are tiny, easy, and boring, once you know what the problem is. The hard work is in putting your finger on that problem: narrowing, drilling down, and measuring, measuring, measuring.Apache Hadoop is no exception to this rule. Recently, Cloudera engineers set out to ensure that MapReduce performance in Hadoop 2 (MR2/YARN) is on par with, or better than, MapReduce performance in Hadoop 1 (MR1). Architecturally, MR2 has many performance advantages over MR1:Better scalability by splitting the JobTracker into the ResourceManager and Application Masters.Better cluster utilization and higher throughput through finer-grained resource scheduling.Less tuning required to avoid over-spilling from smarter sort buffer management.Faster completion times for small jobs through Uber Application Masters, which run all of a jobs tasks in a single JVM.While these improvements are important, none of them mean particularly much for well-tuned medium-sized jobs on medium-sized clusters. Whenever a codebase goes through large changes, regressions are likely to seep in.While correctness issues are easy to spot, performance regressions are difficult to catch without rigorous measurement. When we started including MR2 in our performance measurements last year, we noticed that it lagged behind MR1 significantly on nearly every benchmark. Since then, weve done a ton of work  tuning parameters in Cloudera Manager and fixing regressions in MapReduce itself  and can now proudly say that CDH 5 MR2 performs equally well, or better than, MR1 on all our benchmarks.In this post, Ill offer a couple examples of this work as case studies in tracking down the performance regressions of complex (Java) distributed systems.Ensuring a Fair ComparisonEnsuring a fair comparison between MR1 and MR2 is tricky. One common pitfall is that TeraSort, the job most commonly used for benchmarking, changed between MR1 and MR2. To reflect rule changes in the GraySort benchmark on which it is based, the data generated by the TeraSort included with MR2 is less compressible. A valid comparison would use the same version of TeraSort for both releases; otherwise, MR1 will have an unfair advantage.Another difficult area is resource configuration. In MR1, each nodes resources must be split between slots available for map tasks and slots available for reduce tasks. In MR2, the resource capacity configured for each node is available to both map and reduce tasks. So, if you give MR1 nodes 8 map slots and 8 reduce slots and give MR2 nodes 16 slots worth of memory, resources will be underutilized during MR1s map phase. MR2 will be able to run 16 concurrent mappers per node while MR1 will only be able to run 8. If you only give MR2 nodes 8 slots of memory, then MR2 will suffer during the period when the map and reduce phases overlap  it will only get to run 8 tasks concurrently, while MR1 will be able to run more. (See this post for more information about properly configuring MR2.)To circumvent this issue, our benchmarks give full node capacity in MR1 to both map slots and reduce slots.We then set the mapred.reduce.slowstart.completedmaps parameter in both to .99, meaning that there will be no overlap between the map and reduce phases. This ensures that MR1 and MR2 get full cluster resources for both phases.Case 1: CPU Cache Locality in Sorting Map OutputsA performance fix starts with noticing a performance problem. In this case, I noticed WordCount was lagging: A job that ran in 375 seconds on an MR1 cluster took 472 seconds on an MR2 cluster, more than 25 percent longer.A good start when diagnosing a MapReduce performance issue is to determine in which phase it occurs. In this case, the web UI reported that MR2 map tasks were taking much more time than MR1 map tasks. The next thing is to look for any big differences in the counters; here, they were nearly the same between jobs, so that provided few hints.With little to go on from the counters, the next step is to isolate the problem  reproduce it with as little else going on as possible. The difference in map time showed up when running the same job with a single map and single reduce task in the LocalJobRunner. However, with the reduce task cut out, the times evened out.Because the sort is skipped when no reduce tasks run, it seemed likely that there was some kind of regression in the map-side sort phase.The Map-Side SortThe next step requires a little bit of background on how the map-side sort works.Map output data is placed in an in-memory buffer. When the buffer fills up, the framework sorts it and then writes it to disk (spills it). A separate thread merges the sorted on-disk files into a single larger sorted file. The buffer consists of two parts: a section with contiguous raw output data and a metadata section that holds pointers for each record into the raw data section. In MR1, the sizes of these sections were fixed, controlled by io.sort.record.percent, which could be configured per job.This meant that, without proper tuning of this parameter, if a job had many small records, the metadata section could fill up much more quickly than the raw data section. The buffer would be spilled to disk before it was entirely full.MAPREDUCE-64 fixed this issue in MR2 by allowing the two sections to share the same space and vary in size, meaning that manual tuning of io.sort.record.percent is no longer required to minimize the number of spills.With all this in mind, I realized that we had not yet tuned io.sort.record.percent for the job, and therefore the MR1 map tasks were spilling 10 times as many times as the MR2 map tasks. When I did tune the parameter for MR1 so that it would spill as many times as MR2, MR1 performance actually suffered  spilling fewer larger chunks meant slower map tasks.I had a theory that CPU cache latency was involved. A smaller chunk of output data might fit into a CPU cache, meaning that all the memory accesses when sorting it would be extremely fast. A larger chunk would not fit, meaning that memory accesses would have to go into a higher-level cache or maybe even all the way to main memory. As memory accesses to each cache level take about an order of magnitude longer, this could cause a large performance hit.Fortunately, there is an extremely powerful Linux profiling tool, called perf, that makes it easy to measure this. Running perf stat -e cache-misses  will spit out the number of CPU cache misses encountered by the command. In this case, MR2 job and the tuned MR1 job had a similar number of cache misses, while the untuned MR1 job had half as many.Improving CPU Cache LatencySo how to improve CPU cache latency? At the suggestion of Todd Lipcon, I took a look at MAPREDUCE-3235, an unfinished JIRA from a year ago that proposed a couple ways to improve CPU cache performance in exactly this situation. The change suggested was trivial: Instead of sorting indices into the map output meta array, sort the array itself. Previously, to access the nth map output record, I found the nth element in the index into the meta array, followed that to the entry in the meta array, and then followed the position reported there into the raw output data. Now, I just find the nth element in the meta array and follow the position reported there.This approach removes a layer of indirection and means that I need to access fewer possibly far away memory locations. The drawback is that when the sort does a swap, it moves the full metadata entry (about 20 bytes) instead of the index (4 bytes). A memory access outside the cache is far more expensive than an extra move instruction inside the cache, so its worth the cost.The tiny change worked like magic. It cut the number of cache misses in half for the local job and brought the runtime of the MR2 job on the cluster to less than the runtime of the MR1 job.Win!Case 2: Over-reading in the ShuffleThanks to a report from a Cloudera partner, we learned that more disk reads were occurring in the shuffle during an MR2 job than during the same one on MR1. To reproduce this issue, I turned to ShuffleText, a benchmark job we run that specifically targets the shuffle. It generates a bunch of data in the mappers, shuffles it, and then throws it away in the reducers.I failed to reproduce the problem with a pseudo-distributed setup, but it immediately reared its head when I ran the jobs on the cluster. The job submitted to MR2 took 30 percent longer than the same job submitted to MR1. Even more dramatic, the average time spent per reduce task fetching map output data was a whopping 60 seconds in MR2 compared to 27 seconds in MR1.While MapReduce counters are helpful in many situations, they dont provide machine-wide hardware metrics like number of reads that actually hit disk. Cloudera Manager was invaluable for both measuring the problem and allowing us to dig down into what was wrong. I quickly created charts that showed the total bytes read from disk on machines in the cluster while the job was running: 31.2GB for MR2 compared with 6.4GB for MR1.Disk reads happen in a few different places in a MapReduce job: First, when reading the input data from disk; second, when merging map outputs if there are multiple spills; third, when serving data to reducers during the shuffle; and fourth, when merging data on the reduce side.The TaskTracker and NodeManager processes are responsible for serving intermediate data to reducers in MR1 and MR2, respectively. Looking at the disk bytes read by these processes in Cloudera Manager confirmed that the extra reads were occurring when serving data to the reducers. Looking at the code, I noticed that the MR2 shuffle had been rewritten to serve data with Netty async IO instead of the Jetty web server, which used a thread per request  but nothing popped out as to why this specific issue could be occurring. Adding log messages that printed out the total number of bytes read by the shuffle server yielded no useful leads.On top of this I noticed a few facts:In both MR1 and MR2, the sum of bytes being read from disk during the shuffle phase was smaller than the total map output bytes.In MR1, the majority of shuffle disk reads appeared to be occurring on two of the machines, while the rest had nearly 0.These two machines happened to have less physical memory than the other machines on the cluster.The story that best fit all three was that, in MR1, most of the map outputs were able to reside in memory in the OS buffer cache. The two machines were experiencing more disk reads because their smaller physical memory meant they couldnt fit the map output data in the cache. For some reason, MR2 was not taking advantage of it as well as MR1.fadviseTurning again to the code, I noticed that MR (both 1 and 2) is very careful about its interactions with the OS buffer cache. The Linux fadvise system call allows providing the OS memory subsystem with suggestions about whether or not to cache regions of files. When the shuffle server receives a request for map output data, it fadvises file regions that it is about to read with FADV_WILLNEED so that they will be ready in memory. When done with a file region, the server fadvises it out of memory with FADV_DONTNEED to free up space because it knows that the region likely will not be consumed again.Without any obvious bad logic in the code, the next step was to try figuring out more directly what was going on. I turned to strace, which tracks all the system calls made by a process, and listened for fadvises, file opens, closes, and reads. The amount of data produced by this proved unmanageable to sort by hand. What about simply counting the number of WILLNEEDs and DONTNEEDs?The WILLNEEDs had pretty similar numbers between MR1 and MR2, but seemed to vary per run. For the DONTNEEDs, MR1 was pegged at 768 per node, which corresponded exactly to the number of map tasks that ran on that node multiplied by the number of reducers. But MR2 was always higher than this; and varied between runs.A mere four hours of aggravation later, it all fell into place: In normal operation, reducers will often issue requests for map outputs and then decide they dont want these outputs yet and terminate the connection. This occurs because reducers have a limited amount of memory into which read map outputs, and they find out how much space a map output will take up from a prefix in the response when they request it from the NodeManager/TaskTracker. (In other words, they dont know how much space a map output will take up before asking for it, so, when they find out, they may need to abort and come back later when the space is available.)If the reducer terminates a connection, the shuffle server should not evict the file regions being fetched from the OS cache (not fadvise them as DONTNEED) because the reducer will come back and ask for them later. MR1 was doing this right. MR2 was not, meaning that even if a map output was in memory the first time the fetcher came around, it would be evicted if the reducer terminated the connection, and when the reducer came back it would need to be read from disk.The fix merely consisted of the shuffle server not fadvise-ing regions as DONTNEED when a fetch terminates before reading the whole data. This resulted in the average time a reducer spends fetching intermediate data dropping from 60 seconds to 27, the same as MR1. The average job run time also dropped by 30 percent, bringing it in line with MR1 as well.The astute reader will realize that the situation could be improved even further by communicating the size of map output regions to reducers before they try to fetch them. This would allow us to avoid initiating a fetch when the reducer cant fit the results inside its merge buffer, and reduce unnecessary seeks on the shuffle server side. (We would like to implement this change in future work.)ConclusionI hope that these experiences will assist you in your own performance regression testing, and maybe give you a tiny drop of solace the next time youre trapped inside an Eclipse window, wondering whether theres a way to make everything ok.Thanks to these improvements and many others from Cloudera and the rest of the community (the MR2 improvements have gone upstream and will appear in Hadoop 2.3), we are confident that the version of MR2 that will ship inside CDH 5 (in beta at the time of writing; download here) will perform at least as well, and very likely better, than MR1!Many thanks to Yanpei Chen, Prashant Gokhale, Todd Lipcon, and Chris Leroy for their assistance on this project.Sandy Ryza is a Software Engineer at Cloudera and a Hadoop Committer.This FAQ contains answers to the most frequently asked questions about the architecture and configuration choices involved.In December 2013, Cloudera and Amazon Web Services (AWS) announced a partnership to support Cloudera Enterprise on AWS infrastructure. Along with this announcement, we released a Deployment Reference Architecture Whitepaper. In this post, youll get answers to the most frequently asked questions about the architecture and the configuration choices that have been highlighted in that whitepaper.For what workloads is this deployment model designed?This reference architecture is intended for long-running Cloudera Enterprise Data Hub Edition (EDH) clusters, where the source of data for your workloads is HDFS with S3 as a secondary storage system (used for backups). Different kinds of workloads can run in this environment, including batch processing (MapReduce), fast in-memory analytics (Apache Spark), interactive SQL (Impala), search, and low-latency serving using HBase.This deployment model is not designed for transient workloads such as spinning up a cluster, running a MapReduce job to process some data, and spinning it down; that model involves different considerations and design. Clusters with workflow-defined lifetimes (transient clusters) will be addressed in a future publication of the reference architecture.Why support only VPC deployments?Amazon Virtual Public Cloud (VPC) is the standard deployment model for AWS resources and the default for all new accounts that are being created now. Cloudera recommends deploying in VPC for the following reasons:The easiest way to deploy in AWS, where the AWS resources appear as an extension to the corporate network, is to do so inside a VPC, with a VPN/Direct Connect link to the particular AZ in which you are deploying.VPC has more advanced security options that you can use to comply with security policies.More advanced features, and better network performance for new instance types, are available.Should I have one VPC per cluster? Or should I have one subnet per cluster in a single VPC? What about multiple clusters in a single subnet?Some customers consider having one VPC per environment (dev, QA, prod). Within a single VPC, you can have independent subnets for different clusters  and in some cases, multiple subnets for each cluster, where each subnet is for instances playing a particular role (such as Flume nodes, cluster nodes, and so on).The easiest way to deploy a cluster is to deploy all nodes to a single subnet and use security groups to control ingress and egress in a single VPC. Keep in mind that its nontrivial to get instances in different VPCs to interact.What about different subnets for different roles versus controlling access using security groups?You have two models of deployment to consider, depending on your security requirements and policies:Entire cluster within a single subnet  this means that all the different role types that make up a cluster (slaves, masters, Flume nodes, edge nodes) will be deployed within a single subnet. In most cases, the network access rules for these nodes differ. For example, users will be allowed to login to the edge nodes but not the slave or the master nodes. When deploying in a single subnet, the network rules can be modeled using security groups.Subnet per role per cluster  in this model, each of the different roles will have its own subnet in which to deploy. This is a more complex network topology and allows for finer-grained control over the network rules. In this case, you can use a combination of subnet route tables, security groups, and network ACLs to define your networking rules. However, just using security groups and defining the route tables appropriately is sufficient from a functionality standpoint.Both models are equally valid, but Model #1 is easier to manage.I dont want my instances to be accessible from the Internet. Do I HAVE to deploy them in a public subnet?Currently, there are two ways an instance can get outbound access to the internet, which is required for it to access other AWS services like S3 (excluding RDS) or external repositories for software updates (find detailed documentation here):By having a public IP address  this allows the instance to initiate outgoing requests. You can block all incoming traffic using Network ACLs or Security Groups. In this case, you have to set up the routing within your VPC to permit traffic between the subnet hosting your instances and the Internet gateway.By having a private IP address only but having a NAT instance in a different subnet through which to route traffic  this allows for all traffic to be routed through the NAT instance. Similar to on-premise configurations, a NAT instance is typically a Linux EC2 instance configured to run as a NAT residing in a subnet that has access to the Internet. You can direct public Internet traffic from subnets that cant directly access the Internet to the NAT instance.If you just transfer any sizable amount of data to the public Internet domain (including S3), the recommended method is deployment Model 1. With Model 2, you will bottleneck on the NAT instance.Why only choose cc2.8xlarge and hs1.8xlarge instances as the supported ones?Cloudera Enterprise Data Hub Edition deployments have multiple kinds of workloads running in a long running cluster. To support these different workloads, the individual instances need to provide enough horsepower. The cc2.8xlarge and hs1.8xlarge instances make for the best choices amongst all EC2 instances for such deployments for the following reasons:Individual instance performance does not suffer from the problem of chatty neighboring applications on the same physical host.These instances are on a flat 10G network.They have a good amount of CPU and RAM available.For relatively low storage density requirements, the cc2.8xlarge are the recommended option, and where the storage requirement is high, the hs1.8xlarge are a better choice.Other instance types are reasonable options for specialized workloads and use cases. For example, a memcached deployment would likely benefit from the high-memory instances, and a transient cluster with only batch-processing requirements could probably leverage the m1 family instances (while having a higher number of them). However, as previously explained, those workloads are not addressed by this reference architecture, which is rather intended for long-running EDH deployments where the primary storage is HDFS on the instance stores, supporting multiple different kinds of workloads on the same cluster.Why not EBS-backed HDFS?There are multiple reasons why some people consider Amazon Elastic Block Storage (EBS). They include:Increasing the storage density per node but using smaller instance typesYou can certainly increase the storage density per node by mounting EBS volumes. Having said that, there are a few reasons why doing so doesnt help:Not many of the instance types are good candidates for running an EDH that can sustain different kinds of workloads predictably. Adding a bunch of network-attached storage does theoretically increase the storage capacity, but the other resources like CPU, memory, and network bandwidth dont change. Therefore, its undesirable to use small instance types with EBS volumes attached to them.The bandwidth between the EC2 instances and EBS volumes is limited so youll likely be bottlenecked on that.EBS shines with random I/O. Sequential I/O, which is the predominant access pattern for Hadoop, is not EBSs forte.You pay per IOP on EBS, and for workloads that require large amounts of I/O, that can get expensive to a point that having more instances might be more reasonable than adding EBS volumes and keeping the instance footprint small.Allowing expansion of storage on existing instances in an existing cluster, thereby not having to add more instances if the storage requirements increase.The justifications for this requirement are similar to those above. Furthermore, adding storage to a cluster that is predominantly backed by the instance-stores would mean that you have heterogeneous storage options in the same cluster, with different performance and operational characteristics.More EBS volumes means more spindles, and hence better performance.Adding EBS volumes does not necessarily mean better I/O performance. For example, EBS volumes are network attached  therefore, the performance is limited by the network bandwidth between the EC2 instances and EBS. Furthermore, as highlighted previously, EBS shines with random I/O in contrast to sequential I/O, which is the predominant access pattern for Hadoop.Storing the actual files in EBS will enable pausing the cluster and bringing it back on at a later stage.Today, this is a complex requirement from an operational perspective. The only EC2 instances that can be stopped and later restarted are the EBS-backed ones; the others can only be terminated.If you mount a bunch of EBS volumes to the EBS-backed instances and use them as data directories, they remain there when the instances are started up again and the data in them stays intact. From that perspective, youll have all your data directories mounted just the way you left them prior to the restart, and HDFS should be able to resume operations.If you mount EBS volumes onto instance-store backed instances, restarting HDFS would mean un-mounting all the EBS volumes when you stop a cluster and then re-mounting them onto a new cluster later. This approach is operationally challenging as well as error-prone.Although both these options are plausible in theory, they are also not very well tested, and HDFS is not designed to leverage these features regardless.EBS has higher durability than instance stores and we can reduce the HDFS replication if we use EBS.This is an interesting proposition and the arguments for and against it are the same as if you were to use NAS to back your HDFS on bare-metal deployments. While certainly doable, there are downsides:By reducing replication of HDFS, you are not only giving up on fault tolerance and fast recoverability but also performance. Because fewer copies of the blocks would be available with which to work, more data will move over the network.All your data will be going over the network between the EC2 instance and EBS volumes, thereby affecting performance.Using EBS to back HDFS certainly looks like an attractive option, but as you look at all the factors mentioned above, it should become clear that it has too many performance, cost, and operational drawbacks.Can I pause my cluster in this deployment model? (Or, Can I stop a cluster when Im not using it and save money?)Clusters in which HDFS is backed by instance-stores cannot be paused. Pausing a cluster entails stopping the instances, and when you stop instances, the data on the instance-stores is lost. You can find more information about instance lifecycle here.What if I dont want connectivity back to my data center via VPN or Direct Connect?You dont have to have connectivity back to your data center if you dont have to move data between your Hadoop cluster in AWS and other components that may be hosted in your data center.What are placement groups, and why should I care?As formally defined by AWS documentation:A placement group is a logical grouping of instances within a single Availability Zone. Using placement groups enables applications to get the full-bisection bandwidth and low-latency network performance required for tightly coupled, node-to-node communication typical of HPC applications.By deploying your cluster in a placement group, you are guaranteeing predictable network performance across your instances. Anecdotally, network performance between instances within a single Availability Zone that are not in a single placement group can be lower than if they were within the same placement group. As of today, our recommendation is to spin up your cluster (at least the slave nodes) within a placement group. Having said that, placement groups are more restrictive in terms of the capacity pool that you can use to provision your cluster, which can make expanding a cluster challenging.The root volume is too small for the logs and parcels. What are my choices?You can resize the root volume on instantiation. However, doing so is more challenging with some AMIs than others. The only reason to resize the root volume is to be able to have enough space to store the logs that Hadoop generates, as well as parcels. For those two purposes, our recommendation is to mount an additional EBS volume and use that instead. You can use the additional EBS volume by sym-linking the /var/logs and /opt/cloudera directories to that. You can also configure Cloudera Manager to use a different path than /var/logs for logs and /opt/cloudera for parcels.In a future post, well cover options for backups, high availability, and disaster recovery in the context of deployments in AWS.Amandeep Khurana is a Principal Solutions Architect at Cloudera and has been heavily involved in Clouderas cloud efforts.