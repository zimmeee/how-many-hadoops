Understanding how checkpointing works in HDFS can make the difference between a healthy cluster or a failing one. Checkpointing is an essential part of maintaining and persisting filesystem metadata in HDFS. It's crucial for efficient NameNode recovery and restart, and is an important indicator of overall cluster health. However, checkpointing can also be a source of confusion for operators of Apache Hadoop clusters.  In this post, I'll explain the purpose of checkpointing in HDFS, the technical details of how checkpointing works in different cluster configurations, and then finish with a set of operational concerns and important bug fixes concerning this feature.  Filesystem Metadata in HDFS  To start, let's first cover how the NameNode persists filesystem metadata.    HDFS metadata changes are persisted to the edit log.  At a high level, the NameNode's primary responsibility is storing the HDFS namespace. This means things like the directory tree, file permissions, and the mapping of files to block IDs. It's important that this metadata (and all changes to it) are safely persisted to stable storage for fault tolerance.  This filesystem metadata is stored in two different constructs: the fsimage and the edit log. The fsimage is a file that represents a point-in-time snapshot of the filesystem's metadata. However, while the fsimage file format is very efficient to read, it's unsuitable for making small incremental updates like renaming a single file. Thus, rather than writing a new fsimage every time the namespace is modified, the NameNode instead records the modifying operation in the edit log for durability. This way, if the NameNode crashes, it can restore its state by first loading the fsimage then replaying all the operations (also called edits or transactions) in the edit log to catch up to the most recent state of the namesystem. The edit log comprises a series of files, called edit log segments, that together represent all the namesystem modifications made since the creation of the fsimage.  As an aside, this pattern of using a log for incremental changes on top of another storage format is quite common for traditional filesystems. Log-structured filesystems take this to an extreme and use just a log for persisting data, but more common journaling filesystems like EXT3, EXT4, and XFS support writing changes to a journal before applying them to their final locations on disk.  Why is Checkpointing Important?  A typical edit ranges from 10s to 100s of bytes, but over time enough edits can accumulate to become unwieldy. A couple of problems can arise from these large edit logs. In extreme cases, it can fill up all the available disk capacity on a node, but more subtly, a large edit log can substantially delay NameNode startup as the NameNode reapplies all the edits. This is where checkpointing comes in.  Checkpointing is a process that takes an fsimage and edit log and compacts them into a new fsimage. This way, instead of replaying a potentially unbounded edit log, the NameNode can load the final in-memory state directly from the fsimage. This is a far more efficient operation and reduces NameNode startup time.    Checkpointing creates a new fsimage from an old fsimage and edit log.  However, creating a new fsimage is an I/O- and CPU-intensive operation, sometimes taking minutes to perform. During a checkpoint, the namesystem also needs to restrict concurrent access from other users. So, rather than pausing the active NameNode to perform a checkpoint, HDFS defers it to either the SecondaryNameNode or Standby NameNode, depending on whether NameNode high-availability is configured. The mechanics of checkpointing differs depending on if NameNode high-availability is configured; we'll cover both.  In either case though, checkpointing is triggered by one of two conditions: if enough time has elapsed since the last checkpoint (dfs.namenode.checkpoint.period), or if enough new edit log transactions have accumulated (dfs.namenode.checkpoint.txns). The checkpointing node periodically checks if either of these conditions are met (dfs.namenode.checkpoint.check.period), and if so, kicks off the checkpointing process.  Checkpointing with a Standby NameNode  Checkpointing is actually much simpler when dealing with an HA setup, so let's cover that first.  When NameNode high-availability is configured, the active and standby NameNodes have a shared storage where edits are stored. Typically, this shared storage is an ensemble of three or more JournalNodes, but that's abstracted away from the checkpointing process.  The standby NameNode maintains a relatively up-to-date version of the namespace by periodically replaying the new edits written to the shared edits directory by the active NameNode. As a result, checkpointing is as simple as checking if either of the two preconditions are met, saving the namespace to a new fsimage (roughly equivalent to running `hdfs dfsadmin -saveNamespace` on the command line), then transferring the new fsimage to the active namenode via HTTP.    Checkpointing with NameNode HA configured  Here, Standby NameNode is abbreviated as SbNN and Active NameNode as ANN:  SbNN checks whether either of the two preconditions are met: elapsed time since the last checkpoint or number of accumulated edits. SbNN saves its namespace to an a new fsimage with the intermediate name fsimage.ckpt_, where txid is the transaction ID of the most recent edit log transaction. Then, the SbNN writes an MD5 file for the fsimage, and renames the fsimage to fsimage_. While this is taking place, most other SbNN operations are blocked. This means administrative operations like NameNode failover or accessing parts of the SbNN's webui. Routine HDFS client operations (such as listing, reading, and writing files) are unaffected as these operations are serviced by the ANN. SbNN sends an HTTP GET to the active NN's GetImageServlet at /getimage?putimage=1. The URL parameters also have the transaction ID of the new fsimage and the SbNN's hostname and HTTP port. The active NN's servlet uses the information in the GET request to in turn do its own GET back to the SbNN's GetImageServlet. Similar to the standby, it first saves the new fsimage with the intermediate name fsimage.ckpt_, creates the MD5 file for the fsimage, and then renames the new fsimage to fsimage_. Checkpointing with a SecondaryNameNode  In a non-HA deployment, checkpointing is done on the SecondaryNameNode rather than the standby NameNode. Since there isn't a shared edits directory or automatic tailing of the edit log, the SecondaryNameNode has to go through a few more steps first to refresh its view of the namespace before continuing down the same basic steps.    Time diagram of 2NN and NN with annotated steps  Here, the NameNode is abbreviated as NN and the SecondaryNameNode as 2NN:  2NN checks whether either of the two preconditions are met: elapsed time since the last checkpoint or number of accumulated edits. In the absence of a shared edit directory, the most recent edit log transaction ID needs to be queried via an explicit RPC to the NameNode (NamenodeProtocol#getTransactionId). 2NN triggers an edit log roll, which ends the current edit log segment and starts a new one. The NN can keep writing edits to the new segment while the SNN compacts all the previous ones. This also returns the transaction IDs of the current fsimage and the edit log segment that was just rolled. Explicit triggering of an edit log roll is not necessary in an HA configuration, since the standby NameNode periodically rolls the edit log orthogonal to checkpointing. Given these two transaction IDs, the 2NN fetches new fsimage and edit files as needed via GET to the NN's GetImageServlet. The 2NN might already have some of these files from a previous checkpoint (such as the current fsimage). If necessary, the 2NN reloads its namespace from a newly downloaded fsimage. The 2NN replays the new edit log segments to catch up to the current transaction ID. From here, the rest is the same as in the HA case with a StandbyNameNode.  2NN writes out its namespace to a new fsimage. The 2NN contacts the NN via HTTP GET at /getimage?putimage=1, causing the NN's servlet to do its own GET to the 2NN to download the new fsimage. Operational Implications  The biggest operational concern related to checkpointing is when it fails to happen. We've seen scenarios where NameNodes accumulated hundreds of GBs of edit logs, and no one noticed until the disks filled completely and crashed the NN. When this happens, there's not much to do besides restart the NN and wait for it to replay all the edits. Because of the potential severity of this issue, Cloudera Manager will warn if the current fsimage is out of date, if the checkpointing 2NN or SbNN is down, as well as if NN disks are close to capacity.  Checkpointing is a very I/O and network intensive operation and can affect client performance. This is especially true on a large cluster with millions of files and a multi-GB fsimage, since copying a new fsimage to the NameNode can eat up all available bandwidth. In this case, the transfer speed can be throttled with dfs.image.transfer.bandwidthPerSec. If you do adjust this parameter, you might also need to adjust dfs.image.transfer.timeout based on your expected transfer time.  If you're on an older version of CDH, there are also a number of issues related to checkpointing that might make upgrading worthwhile.  Spark is a compelling multi-purpose platform for use cases that span investigative, as well as operational, analytics.  Data science is a broad church. I am a data scientist - or so I've been told - but what I do is actually quite different from what other "data scientists" do. For example, there are those practicing "investigative analytics" and those implementing "operational analytics." (I'm in the second camp.)   Data scientists performing investigative analytics use interactive statistical environments like R to perform ad-hoc, exploratory analytics in order to answer questions and gain insights. By contrast, data scientists building operational analytics systems have more in common with engineers. They build software that creates and queries machine-learning models that operate at scale in real-time serving environments, using systems languages like C++ and Java, and often use several elements of an enterprise data hub, including the Apache Hadoop ecosystem.  And there are subgroups within these groups of data scientists. For example, some analysts who are proficient with R have never heard of Python or scikit-learn, or vice versa, even though both provide libraries of statistical functions that are accessible from a REPL (Read-Evaluate-Print Loop) environment.  A World of Tradeoffs  It would be wonderful to have one tool for everyone, and one architecture and language for investigative as well as operational analytics. If I primarily work in Java, should I really need to know a language like Python or R in order to be effective at exploring data? Coming from a conventional data analyst background, must I understand MapReduce in order to scale up computations? The array of tools available to data scientists tells a story of unfortunate tradeoffs:  R offers a rich environment for statistical analysis and machine learning, but it has some rough edges when performing many of the data processing and cleanup tasks that are required before the real analysis work can begin. As a language, it's not similar to the mainstream languages developers know. Python is a general purpose programming language with excellent libraries for data analysis like Pandas and scikit-learn. But like R, it's still limited to working with an amount of data that can fit on one machine. It's possible to develop distributed machine learning algorithms on the classic MapReduce computation framework in Hadoop (see Apache Mahout). But MapReduce is notoriously low-level and difficult to express complex computations in. Apache Crunch offers a simpler, idiomatic Java API for expressing MapReduce computations. But still, the nature of MapReduce makes it inefficient for iterative computations, and most machine learning algorithms have an iterative component. And so on. There are both gaps and overlaps between these and other data science tools. Coming from a background in Java and Hadoop, I do wonder with envy sometimes: why can't we have a nice REPL-like investigative analytics environment like the Python and R users have? That's still scalable and distributed? And has the nice distributed-collection design of Crunch? And can equally be used in operational contexts?  Common Ground in Spark  These are the desires that make me excited about Apache Spark. While discussion about Spark for data science has mostly noted its ability to keep data resident in memory, which can speed up iterative machine learning workloads compared to MapReduce, this is perhaps not even the big news, not to me. It does not solve every problem for everyone. However, Spark has a number of features that make it a compelling crossover platform for investigative as well as operational analytics:  Spark comes with a machine-learning library, MLlib, albeit bare bones so far. Being Scala-based, Spark embeds in any JVM-based operational system, but can also be used interactively in a REPL in a way that will feel familiar to R and Python users. For Java programmers, Scala still presents a learning curve. But at least, any Java library can be used from within Scala. Spark's RDD (Resilient Distributed Dataset) abstraction resembles Crunch's PCollection, which has proved a useful abstraction in Hadoop that will already be familiar to Crunch developers. (Crunch can even be used on top of Spark.) Spark imitates Scala's collections API and functional style, which is a boon to Java and Scala developers, but also somewhat familiar to developers coming from Python. Scala is also a compelling choice for statistical computing. Spark itself, and Scala underneath it, are not specific to machine learning. They provide APIs supporting related tasks, like data access, ETL, and integration. As with Python, the entire data science pipeline can be implemented within this paradigm, not just the model fitting and analysis. Code that is implemented in the REPL environment can be used mostly as-is in an operational context. Data operations are transparently distributed across the cluster, even as you type. Spark, and MLlib in particular, still has a lot of growing to do. For example, the project needs optimizations, fixes, and deeper integration with YARN. It doesn't yet provide nearly the depth of library functions that conventional data analysis tools do. But as a best-of-most-worlds platform, it is already sufficiently interesting for a data scientist of any denomination to look at seriously.  In Action: Tagging Stack Overflow Questions  A complete example will give a sense of using Spark as an environment for transforming data and building models on Hadoop. The following example uses a dump of data from the popular Stack Overflow Q&A site. On Stack Overflow, developers can ask and answer questions about software. Questions can be tagged with short strings like "java" or "sql". This example will build a model that can suggest new tags to questions based on existing tags, using the alternating least squares (ALS) recommender algorithm; questions are "users" and tags are "items".  Getting the Data  Stack Exchange provides complete dumps of all data, most recently from January 20, 2014. The data is provided as a torrent containing different types of data from Stack Overflow and many sister sites. Only the file stackoverflow.com-Posts.7z needs to be downloaded from the torrent.  This file is just a bzip-compressed file. Spark, like Hadoop, can directly read and split some compressed files, but in this case it is necessary to uncompress a copy on to HDFS. In one step, that's:  bzcat stackoverflow.com-Posts.7z | hdfs dfs -put - /user/srowen/Posts.xml    Uncompressed, it consumes about 24.4GB, and contains about 18 million posts, of which 2.1 million are questions. These questions have about 9.3 million tags from approximately 34,000 unique tags.  Set Up Spark  Given that Spark's integration with Hadoop is relatively new, it can be time-consuming to get it working manually. Fortunately, CDH hides that complexity by integrating Spark and managing setup of its processes. Spark can be installed separately with CDH 4.6.0, and is included in CDH 5 Beta 2. This example uses an installation of CDH 5 Beta 2.  This example uses MLlib, which uses the jblas library for linear algebra, which in turn calls native code using LAPACK and Fortran. At the moment, it is necessary to manually install the Fortran library dependency to enable this. The package is called libgfortran or libgfortran3, and should be available from the standard package manager of major Linux distributions. For example, for RHEL 6, install it with:  sudo yum install libgfortran    This must be installed on all machines that have been designated as Spark workers.  Log in to the machine designated as the Spark master with ssh. It will be necessary, at the moment, to ask Spark to let its workers use a large amount of memory. The code in MLlib that is used in this example, in version 0.9.0, has a memory issue, one that is already fixed for the next release. To configure for more memory and launch the shell:  export SPARK_JAVA_OPTS="-Dspark.executor.memory=8g" spark-shell    Interactive Processing in the Shell  The shell is the Scala REPL. It's possible to execute lines of code, define methods, and in general access any Scala or Spark functionality in this environment, one line at a time. You can paste the following steps into the REPL, one by one.  First, get a handle on the Posts.xml file:  val postsXML = sc.textFile("hdfs:///user/srowen/Posts.xml")    In response the REPL will print:  postsXML: org.apache.spark.rdd.RDD[String] = MappedRDD[1] at textFile at :12    The text file is an RDD (Resilient Distributed Dataset) of Strings, which are the lines of the file. You can query it by calling methods of the RDD class. For example, to count the lines:  postsXML.count    This command yields a great deal of output from Spark as it counts lines in a distributed way, and finally prints 18066983.  The next snippet transforms the lines of the XML file into a collection of (questionID,tag) tuples. This demonstrates Scala's functional programming style, and other quirks. (Explaining them is out of scope here.)    In December 2013, Cloudera and Amazon Web Services (AWS) announced a partnership to support Cloudera Enterprise on AWS infrastructure. Along with this announcement, we released a Deployment Reference Architecture Whitepaper. In this post, you'll get answers to the most frequently asked questions about the architecture and the configuration choices that have been highlighted in that whitepaper.  For what workloads is this deployment model designed?  This reference architecture is intended for long-running Cloudera Enterprise Data Hub Edition (EDH) clusters, where the source of data for your workloads is HDFS with S3 as a secondary storage system (used for backups). Different kinds of workloads can run in this environment, including batch processing (MapReduce), fast in-memory analytics (Apache Spark), interactive SQL (Impala), search, and low-latency serving using HBase.  This deployment model is not designed for transient workloads such as spinning up a cluster, running a MapReduce job to process some data, and spinning it down; that model involves different considerations and design. Clusters with workflow-defined lifetimes (transient clusters) will be addressed in a future publication of the reference architecture.  Why support only VPC deployments?  Amazon Virtual Public Cloud (VPC) is the standard deployment model for AWS resources and the default for all new accounts that are being created now. Cloudera recommends deploying in VPC for the following reasons:  The easiest way to deploy in AWS, where the AWS resources appear as an extension to the corporate network, is to do so inside a VPC, with a VPN/Direct Connect link to the particular AZ in which you are deploying. VPC has more advanced security options that you can use to comply with security policies. More advanced features, and better network performance for new instance types, are available. Should I have one VPC per cluster? Or should I have one subnet per cluster in a single VPC? What about multiple clusters in a single subnet?  Some customers consider having one VPC per environment (dev, QA, prod). Within a single VPC, you can have independent subnets for different clusters - and in some cases, multiple subnets for each cluster, where each subnet is for instances playing a particular role (such as Flume nodes, cluster nodes, and so on).  The easiest way to deploy a cluster is to deploy all nodes to a single subnet and use security groups to control ingress and egress in a single VPC. Keep in mind that it's nontrivial to get instances in different VPCs to interact.  What about different subnets for different roles versus controlling access using security groups?  You have two models of deployment to consider, depending on your security requirements and policies:  Entire cluster within a single subnet - this means that all the different role types that make up a cluster (slaves, masters, Flume nodes, edge nodes) will be deployed within a single subnet. In most cases, the network access rules for these nodes differ.  For example, users will be allowed to login to the edge nodes but not the slave or the master nodes. When deploying in a single subnet, the network rules can be modeled using security groups. Subnet per role per cluster - in this model, each of the different roles will have its own subnet in which to deploy. This is a more complex network topology and allows for finer-grained control over the network rules. In this case, you can use a combination of subnet route tables, security groups, and network ACLs to define your networking rules. However, just using security groups and defining the route tables appropriately is sufficient from a functionality standpoint. Both models are equally valid, but Model #1 is easier to manage.  I don't want my instances to be accessible from the Internet. Do I HAVE to deploy them in a public subnet?  Currently, there are two ways an instance can get outbound access to the internet, which is required for it to access other AWS services like S3 (excluding RDS) or external repositories for software updates (find detailed documentation here):  By having a public IP address - this allows the instance to initiate outgoing requests. You can block all incoming traffic using Network ACLs or Security Groups. In this case, you have to set up the routing within your VPC to permit traffic between the subnet hosting your instances and the Internet gateway. By having a private IP address only but having a NAT instance in a different subnet through which to route traffic - this allows for all traffic to be routed through the NAT instance. Similar to on-premise configurations, a NAT instance is typically a Linux EC2 instance configured to run as a NAT residing in a subnet that has access to the Internet.  You can direct public Internet traffic from subnets that can't directly access the Internet to the NAT instance. If you just transfer any sizable amount of data to the public Internet domain (including S3), the recommended method is deployment Model 1. With Model 2, you will bottleneck on the NAT instance.  Why only choose cc2.8xlarge and hs1.8xlarge instances as the supported ones?  Cloudera Enterprise Data Hub Edition deployments have multiple kinds of workloads running in a long running cluster. To support these different workloads, the individual instances need to provide enough horsepower. The cc2.8xlarge and hs1.8xlarge instances make for the best choices amongst all EC2 instances for such deployments for the following reasons:  Individual instance performance does not suffer from the problem of chatty neighboring applications on the same physical host. These instances are on a flat 10G network. They have a good amount of CPU and RAM available. For relatively low storage density requirements, the cc2.8xlarge are the recommended option, and where the storage requirement is high, the hs1.8xlarge are a better choice.  Other instance types are reasonable options for specialized workloads and use cases. For example, a memcached deployment would likely benefit from the high-memory instances, and a transient cluster with only batch-processing requirements could probably leverage the m1 family instances (while having a higher number of them). However, as previously explained, those workloads are not addressed by this reference architecture, which is rather intended for long-running EDH deployments where the primary storage is HDFS on the instance stores, supporting multiple different kinds of workloads on the same cluster.  Why not EBS-backed HDFS?  There are multiple reasons why some people consider Amazon Elastic Block Storage (EBS). They include:  Increasing the storage density per node but using smaller instance types  You can certainly increase the storage density per node by mounting EBS volumes. Having said that, there are a few reasons why doing so doesn't help:  Not many of the instance types are good candidates for running an EDH that can sustain different kinds of workloads predictably. Adding a bunch of network-attached storage does theoretically increase the storage capacity, but the other resources like CPU, memory, and network bandwidth don't change. Therefore, it's undesirable to use small instance types with EBS volumes attached to them. The bandwidth between the EC2 instances and EBS volumes is limited so you'll likely be bottlenecked on that. EBS shines with random I/O. Sequential I/O, which is the predominant access pattern for Hadoop, is not EBS's forte. You pay per IOP on EBS, and for workloads that require large amounts of I/O, that can get expensive to a point that having more instances might be more reasonable than adding EBS volumes and keeping the instance footprint small. Allowing expansion of storage on existing instances in an existing cluster, thereby not having to add more instances if the storage requirements increase.  The justifications for this requirement are similar to those above. Furthermore, adding storage to a cluster that is predominantly backed by the instance-stores would mean that you have heterogeneous storage options in the same cluster, with different performance and operational characteristics.  More EBS volumes means more spindles, and hence better performance.  Adding EBS volumes does not necessarily mean better I/O performance. For example, EBS volumes are network attached - therefore, the performance is limited by the network bandwidth between the EC2 instances and EBS. Furthermore, as highlighted previously, EBS shines with random I/O in contrast to sequential I/O, which is the predominant access pattern for Hadoop.  Storing the actual files in EBS will enable pausing the cluster and bringing it back on at a later stage.  Today, this is a complex requirement from an operational perspective. The only EC2 instances that can be stopped and later restarted are the EBS-backed ones; the others can only be terminated.  If you mount a bunch of EBS volumes to the EBS-backed instances and use them as data directories, they remain there when the instances are started up again and the data in them stays intact. From that perspective, you'll have all your data directories mounted just the way you left them prior to the restart, and HDFS should be able to resume operations.  If you mount EBS volumes onto instance-store backed instances, restarting HDFS would mean un-mounting all the EBS volumes when you stop a cluster and then re-mounting them onto a new cluster later. This approach is operationally challenging as well as error-prone.  Although both these options are plausible in theory, they are also not very well tested, and HDFS is not designed to leverage these features regardless.  EBS has higher durability than instance stores and we can reduce the HDFS replication if we use EBS.  This is an interesting proposition and the arguments for and against it are the same as if you were to use NAS to back your HDFS on bare-metal deployments. While certainly doable, there are downsides:  By reducing replication of HDFS, you are not only giving up on fault tolerance and fast recoverability but also performance. Because fewer copies of the blocks would be available with which to work, more data will move over the network. All your data will be going over the network between the EC2 instance and EBS volumes, thereby affecting performance. Using EBS to back HDFS certainly looks like an attractive option, but as you look at all the factors mentioned above, it should become clear that it has too many performance, cost, and operational drawbacks.  Can I pause my cluster in this deployment model? (Or, Can I stop a cluster when I'm not using it and save money?)  Clusters in which HDFS is backed by instance-stores cannot be paused. Pausing a cluster entails stopping the instances, and when you stop instances, the data on the instance-stores is lost. You can find more information about instance lifecycle here.  What if I don't want connectivity back to my data center via VPN or Direct Connect?  You don't have to have connectivity back to your data center if you don't have to move data between your Hadoop cluster in AWS and other components that may be hosted in your data center.  What are "placement groups", and why should I care?  As formally defined by AWS documentation:  A placement group is a logical grouping of instances within a single Availability Zone. Using placement groups enables applications to get the full-bisection bandwidth and low-latency network performance required for tightly coupled, node-to-node communication typical of HPC applications.  By deploying your cluster in a placement group, you are guaranteeing predictable network performance across your instances. Anecdotally, network performance between instances within a single Availability Zone that are not in a single placement group can be lower than if they were within the same placement group. As of today, our recommendation is to spin up your cluster (at least the slave nodes) within a placement group. Having said that, placement groups are more restrictive in terms of the capacity pool that you can use to provision your cluster, which can make expanding a cluster challenging.  The root volume is too small for the logs and parcels. What are my choices?  You can resize the root volume on instantiation. However, doing so is more challenging with some AMIs than others. The only reason to resize the root volume is to be able to have enough space to store the logs that Hadoop generates, as well as parcels. For those two purposes, our recommendation is to mount an additional EBS volume and use that instead. You can use the additional EBS volume by sym-linking the /var/logs and /opt/cloudera directories to that. You can also configure Cloudera Manager to use a different path than /var/logs for logs and /opt/cloudera for parcels.  In a future post, we'll cover options for backups, high availability, and disaster recovery in the context of deployments in AWS.
